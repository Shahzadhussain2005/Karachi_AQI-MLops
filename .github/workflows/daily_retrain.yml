name: Daily Model Retraining

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  retrain-models:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GH_PAT }}
        fetch-depth: 0
    
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install jupyter nbconvert pandas numpy scikit-learn xgboost lightgbm pymongo dnspython requests meteostat
    
    - name: Verify structure
      run: |
        echo "=== Root directory ==="
        ls -la
        echo ""
        echo "=== Scripts directory ==="
        ls -la Scripts/
        echo ""
        echo "=== Scripts/data ==="
        ls -la Scripts/data/ 2>/dev/null || echo "No data folder yet"
        echo ""
        echo "=== Scripts/models ==="
        ls -la Scripts/models/ 2>/dev/null || echo "No models folder yet"
    
    - name: Fetch latest data
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        cd Scripts
        echo "Fetching data..."
        jupyter nbconvert --to script Fetch_latest_data.ipynb --output /tmp/fetch --log-level ERROR || true
        python /tmp/fetch.py || echo "Fetch skipped"
      continue-on-error: true
    
    - name: Clean data
      run: |
        cd Scripts
        echo "Cleaning data..."
        jupyter nbconvert --to script clean_data.ipynb --output /tmp/clean --log-level ERROR || true
        python /tmp/clean.py || echo "Clean skipped"
      continue-on-error: true
    
    - name: Upload to MongoDB
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        cd Scripts
        if [ -f "mongodb_connect.ipynb" ]; then
          echo "Uploading to MongoDB..."
          jupyter nbconvert --to script mongodb_connect.ipynb --output /tmp/mongo --log-level ERROR || true
          python /tmp/mongo.py || echo "MongoDB upload skipped"
        fi
      continue-on-error: true
    
    - name: Train ML models
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        cd Scripts
        echo "========================================="
        echo "Training ML models..."
        echo "Working directory: $(pwd)"
        echo "Expected time: 10-20 minutes"
        echo "========================================="
        
        # Convert notebook to script
        jupyter nbconvert --to script models.ipynb --output /tmp/train --log-level ERROR
        
        # Run training (stay in Scripts directory so paths work)
        python /tmp/train.py
        exit_code=$?
        
        if [ $exit_code -eq 0 ]; then
          echo "‚úÖ Training completed successfully"
        else
          echo "‚ùå Training failed with exit code: $exit_code"
          exit $exit_code
        fi
    
    - name: Verify models were created
      run: |
        echo "========================================="
        echo "Verifying trained models..."
        echo "========================================="
        
        if [ ! -d "Scripts/models" ]; then
          echo "‚ùå ERROR: Scripts/models/ directory does not exist"
          exit 1
        fi
        
        model_count=$(ls Scripts/models/*.pkl 2>/dev/null | wc -l)
        
        echo "Models found: $model_count"
        echo ""
        echo "Model files:"
        ls -lh Scripts/models/*.pkl 2>/dev/null || echo "No .pkl files"
        echo ""
        
        if [ $model_count -eq 0 ]; then
          echo "‚ùå ERROR: No models were saved!"
          exit 1
        fi
        
        echo "‚úÖ SUCCESS: $model_count model files saved"
        
        # Show results file
        if [ -f "Scripts/models/ml_tuned_results.json" ]; then
          echo ""
          echo "Results summary:"
          head -30 Scripts/models/ml_tuned_results.json
        fi
    
    - name: Prepare dashboard
      run: |
        cd Scripts
        if [ -f "data/cleaned_aqi_data_v2.csv" ]; then
          tail -720 data/cleaned_aqi_data_v2.csv > data/historical_aqi.csv
          echo "‚úì Dashboard data prepared"
        else
          echo "‚ö†Ô∏è No data file found"
        fi
      continue-on-error: true
    
    - name: Commit changes
      run: |
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git config user.name "GitHub Actions Bot"
        
        # Add Scripts subdirectories
        git add Scripts/models/ Scripts/data/ 2>/dev/null || true
        
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          timestamp=$(date +'%Y-%m-%d %H:%M UTC')
          git commit -m "ü§ñ Daily retrain: $timestamp"
          git push origin main
          echo "‚úÖ Changes pushed"
        fi
    
    - name: Summary
      if: always()
      run: |
        echo ""
        echo "========================================="
        echo "        PIPELINE SUMMARY"
        echo "========================================="
        
        if [ -d "Scripts/models" ]; then
          count=$(ls Scripts/models/*.pkl 2>/dev/null | wc -l)
          if [ $count -gt 0 ]; then
            echo "‚úÖ SUCCESS: $count models trained"
            echo ""
            echo "Models:"
            ls -1 Scripts/models/*.pkl 2>/dev/null | head -20
          else
            echo "‚ùå FAILED: No models saved"
          fi
        else
          echo "‚ùå FAILED: Scripts/models/ not found"
        fi
        
        echo ""
        echo "Next run: Tomorrow 00:00 UTC"
        echo "========================================="
