name: Daily Model Retraining

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  retrain:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GH_PAT }}
    
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install
      run: pip install pandas numpy scikit-learn xgboost lightgbm pymongo dnspython
    
    - name: Train Best Models
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        python - <<'EOF'
        import pandas as pd
        import numpy as np
        import pickle
        import json
        import os
        from sklearn.preprocessing import RobustScaler
        from sklearn.feature_selection import SelectKBest, mutual_info_regression
        from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
        from sklearn.linear_model import Ridge
        import xgboost as xgb
        import lightgbm as lgb
        
        print("="*70)
        print("TRAINING: XGBoost(24h) + LightGBM(48h) + Ridge(72h)")
        print("="*70)
        
        # Load
        print("\n1. Loading...")
        try:
            from pymongo import MongoClient
            from pymongo.server_api import ServerApi
            client = MongoClient(os.environ['MONGODB_URI'], server_api=ServerApi('1'), serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            df = pd.DataFrame(list(client['aqi_feature_store']['aqi_features'].find({}, {"_id": 0})))
            client.close()
            print(f"   âœ“ MongoDB: {len(df)} rows")
        except:
            df = pd.read_csv('data/cleaned_aqi_data_v2.csv')
            print(f"   âœ“ CSV: {len(df)} rows")
        
        df['time'] = pd.to_datetime(df.get('time', df.get('timestamp')))
        df = df.sort_values('time').reset_index(drop=True)
        
        # Feature engineering
        print("\n2. Feature engineering...")
        
        for lag in [1, 2, 3, 6, 12, 24, 48]:
            if 'aqi' in df.columns:
                df[f'aqi_lag_{lag}h'] = df['aqi'].shift(lag)
            if 'pm2_5' in df.columns:
                df[f'pm25_lag_{lag}h'] = df['pm2_5'].shift(lag)
        
        for window in [3, 6, 12, 24]:
            if 'aqi' in df.columns:
                df[f'aqi_ma_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).mean()
                df[f'aqi_std_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).std()
                df[f'aqi_min_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).min()
                df[f'aqi_max_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).max()
        
        if 'aqi' in df.columns:
            df['aqi_diff_1h'] = df['aqi'].diff(1)
            df['aqi_diff_3h'] = df['aqi'].diff(3)
            df['aqi_diff_24h'] = df['aqi'].diff(24)
        
        if 'hour' in df.columns:
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        
        df['y24'] = df['aqi'].shift(-24)
        df['y48'] = df['aqi'].shift(-48)
        df['y72'] = df['aqi'].shift(-72)
        df = df.dropna(subset=['y24','y48','y72'])
        
        print(f"   âœ“ Rows: {len(df)}")
        
        # Prepare features
        exclude = ['time', 'timestamp', 'y24', 'y48', 'y72',
                   'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',
                   'season', 'weather_condition', 'day_of_week', 'day_of_month', 'is_weekend']
        
        feature_cols = [c for c in df.columns if c not in exclude]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        X = df[numeric_cols].copy()
        missing_pct = X.isnull().sum() / len(X)
        good_cols = missing_pct[missing_pct < 0.3].index.tolist()
        X = X[good_cols].fillna(X[good_cols].mean()).fillna(0)
        
        y24, y48, y72 = df['y24'], df['y48'], df['y72']
        
        print(f"   âœ“ Features: {len(X.columns)}")
        
        # Feature selection
        print("\n3. Selecting best 30 features...")
        selector = SelectKBest(score_func=mutual_info_regression, k=min(30, len(X.columns)))
        selector.fit(X, y24)
        selected_features = X.columns[selector.get_support()].tolist()
        X = X[selected_features]
        
        print(f"   âœ“ Selected: {len(selected_features)}")
        
        # Split
        split = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split], X.iloc[split:]
        y24_train, y24_test = y24.iloc[:split], y24.iloc[split:]
        y48_train, y48_test = y48.iloc[:split], y48.iloc[split:]
        y72_train, y72_test = y72.iloc[:split], y72.iloc[split:]
        
        # Scale
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        os.makedirs('models', exist_ok=True)
        
        # Time series CV
        tscv = TimeSeriesSplit(n_splits=2)
        
        results = {}
        
        # ============================================================
        # 24h: XGBoost with hyperparameter tuning
        # ============================================================
        print("\n4. Training 24h: XGBoost (with tuning)...")
        
        xgb_params = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0]
        }
        
        xgb_search = RandomizedSearchCV(
            xgb.XGBRegressor(random_state=42, n_jobs=-1),
            xgb_params,
            n_iter=10,
            cv=tscv,
            scoring='r2',
            n_jobs=-1,
            random_state=42
        )
        xgb_search.fit(X_train_scaled, y24_train)
        xgb_model = xgb_search.best_estimator_
        xgb_score = xgb_model.score(X_test_scaled, y24_test)
        
        with open('models/xgboost_24h.pkl', 'wb') as f:
            pickle.dump(xgb_model, f)
        
        results['24h'] = {'model': 'xgboost', 'R2': xgb_score, 'params': xgb_search.best_params_}
        
        print(f"   Best params: {xgb_search.best_params_}")
        print(f"   CV RÂ²: {xgb_search.best_score_:.3f}")
        print(f"   Test RÂ²: {xgb_score:.3f}")
        
        # ============================================================
        # 48h: LightGBM with hyperparameter tuning
        # ============================================================
        print("\n5. Training 48h: LightGBM (with tuning)...")
        
        lgb_params = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1],
            'num_leaves': [31, 63],
            'subsample': [0.8, 1.0]
        }
        
        lgb_search = RandomizedSearchCV(
            lgb.LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1),
            lgb_params,
            n_iter=10,
            cv=tscv,
            scoring='r2',
            n_jobs=-1,
            random_state=42
        )
        lgb_search.fit(X_train_scaled, y48_train)
        lgb_model = lgb_search.best_estimator_
        lgb_score = lgb_model.score(X_test_scaled, y48_test)
        
        with open('models/lightgbm_48h.pkl', 'wb') as f:
            pickle.dump(lgb_model, f)
        
        results['48h'] = {'model': 'lightgbm', 'R2': lgb_score, 'params': lgb_search.best_params_}
        
        print(f"   Best params: {lgb_search.best_params_}")
        print(f"   CV RÂ²: {lgb_search.best_score_:.3f}")
        print(f"   Test RÂ²: {lgb_score:.3f}")
        
        # ============================================================
        # 72h: Ridge with hyperparameter tuning
        # ============================================================
        print("\n6. Training 72h: Ridge (with tuning)...")
        
        ridge_params = {
            'alpha': [0.1, 1.0, 10.0, 100.0],
            'solver': ['auto', 'svd', 'saga']
        }
        
        from sklearn.model_selection import GridSearchCV
        ridge_search = GridSearchCV(
            Ridge(),
            ridge_params,
            cv=tscv,
            scoring='r2',
            n_jobs=-1
        )
        ridge_search.fit(X_train_scaled, y72_train)
        ridge_model = ridge_search.best_estimator_
        ridge_score = ridge_model.score(X_test_scaled, y72_test)
        
        with open('models/ridge_72h.pkl', 'wb') as f:
            pickle.dump(ridge_model, f)
        
        results['72h'] = {'model': 'ridge', 'R2': ridge_score, 'params': ridge_search.best_params_}
        
        print(f"   Best params: {ridge_search.best_params_}")
        print(f"   CV RÂ²: {ridge_search.best_score_:.3f}")
        print(f"   Test RÂ²: {ridge_score:.3f}")
        
        # Save artifacts
        with open('models/scaler_ml.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('models/feature_names.json', 'w') as f:
            json.dump(selected_features, f)
        
        with open('models/results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("\n" + "="*70)
        print("âœ… TRAINING COMPLETE!")
        print("="*70)
        print(f"\nResults:")
        print(f"  24h: XGBoost    (RÂ²={xgb_score:.3f})")
        print(f"  48h: LightGBM   (RÂ²={lgb_score:.3f})")
        print(f"  72h: Ridge      (RÂ²={ridge_score:.3f})")
        print(f"\nFeatures: {len(selected_features)}")
        EOF
    
    - name: Show results
      run: |
        echo "=== Models ==="
        ls -lh models/*.pkl
        echo ""
        echo "=== Scores ==="
        cat models/results.json
    
    - name: Commit
      run: |
        git config user.email "bot@github.com"
        git config user.name "Bot"
        git add models/
        git diff --staged --quiet || git commit -m "ðŸš€ Best models $(date +'%Y-%m-%d')" && git push
