name: Daily Model Retraining

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  retrain:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GH_PAT }}
    
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install
      run: pip install pandas numpy scikit-learn xgboost lightgbm pymongo dnspython
    
    - name: Train
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        python - <<'EOF'
        import pandas as pd
        import numpy as np
        import pickle
        import json
        import os
        from sklearn.preprocessing import RobustScaler
        from sklearn.feature_selection import SelectKBest, mutual_info_regression
        import xgboost as xgb
        import lightgbm as lgb
        from sklearn.ensemble import GradientBoostingRegressor
        
        print("="*70)
        print("TRAINING")
        print("="*70)
        
        print("\n1. Loading data...")
        try:
            from pymongo import MongoClient
            from pymongo.server_api import ServerApi
            client = MongoClient(os.environ['MONGODB_URI'], server_api=ServerApi('1'), serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            df = pd.DataFrame(list(client['aqi_feature_store']['aqi_features'].find({}, {"_id": 0})))
            client.close()
            print(f"   âœ“ MongoDB: {len(df)} rows")
        except:
            df = pd.read_csv('data/cleaned_aqi_data_v2.csv')
            print(f"   âœ“ CSV: {len(df)} rows")
        
        df['time'] = pd.to_datetime(df.get('time', df.get('timestamp')))
        df = df.sort_values('time').reset_index(drop=True)
        
        print("\n2. Engineering features...")
        
        # Lag features
        for lag in [1, 3, 6, 12, 24]:
            if 'aqi' in df.columns:
                df[f'aqi_lag_{lag}h'] = df['aqi'].shift(lag)
        
        # Rolling features
        for window in [3, 6, 12, 24]:
            if 'aqi' in df.columns:
                df[f'aqi_ma_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).mean()
                df[f'aqi_std_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).std()
                df[f'aqi_min_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).min()
                df[f'aqi_max_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).max()
        
        # Targets
        df['y24'] = df['aqi'].shift(-24)
        df['y48'] = df['aqi'].shift(-48)
        df['y72'] = df['aqi'].shift(-72)
        
        # Remove rows with missing targets
        df = df.dropna(subset=['y24','y48','y72'])
        
        print(f"   âœ“ After engineering: {len(df)} rows")
        
        # Get numeric features
        exclude = ['time', 'timestamp', 'y24', 'y48', 'y72', 
                   'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',
                   'season', 'weather_condition', 'day_of_week', 'day_of_month', 'is_weekend']
        
        feature_cols = [c for c in df.columns if c not in exclude]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        # CRITICAL FIX: Fill NaN BEFORE feature selection
        X = df[numeric_cols].copy()
        
        # Remove columns with >30% missing
        missing_pct = X.isnull().sum() / len(X)
        good_cols = missing_pct[missing_pct < 0.3].index.tolist()
        X = X[good_cols]
        
        # Fill remaining NaN with mean
        X = X.fillna(X.mean())
        
        # Final check
        if X.isnull().any().any():
            print("   âš ï¸ Still has NaN, filling with 0")
            X = X.fillna(0)
        
        y24, y48, y72 = df['y24'], df['y48'], df['y72']
        
        print(f"   âœ“ Features: {len(X.columns)}")
        print(f"   âœ“ NaN check: {X.isnull().sum().sum()} (should be 0)")
        
        print("\n3. Selecting features...")
        selector = SelectKBest(score_func=mutual_info_regression, k=min(30, len(X.columns)))
        selector.fit(X, y24)
        selected_features = X.columns[selector.get_support()].tolist()
        X = X[selected_features]
        
        print(f"   âœ“ Selected: {len(selected_features)}")
        
        # Split
        split = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split], X.iloc[split:]
        y24_train, y24_test = y24.iloc[:split], y24.iloc[split:]
        y48_train, y48_test = y48.iloc[:split], y48.iloc[split:]
        y72_train, y72_test = y72.iloc[:split], y72.iloc[split:]
        
        # Scale
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        os.makedirs('models', exist_ok=True)
        
        print("\n4. Training...")
        
        for horizon, y_tr, y_te in [
            ('24h', y24_train, y24_test),
            ('48h', y48_train, y48_test),
            ('72h', y72_train, y72_test)
        ]:
            print(f"\n   {horizon}:")
            
            xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, subsample=0.8, random_state=42, n_jobs=-1)
            xgb_model.fit(X_train_scaled, y_tr)
            xgb_score = xgb_model.score(X_test_scaled, y_te)
            
            lgb_model = lgb.LGBMRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, num_leaves=31, random_state=42, verbose=-1, n_jobs=-1)
            lgb_model.fit(X_train_scaled, y_tr)
            lgb_score = lgb_model.score(X_test_scaled, y_te)
            
            gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, subsample=0.8, random_state=42)
            gb_model.fit(X_train_scaled, y_tr)
            gb_score = gb_model.score(X_test_scaled, y_te)
            
            scores = [(xgb_model, 'xgboost', xgb_score), (lgb_model, 'lightgbm', lgb_score), (gb_model, 'gradient_boosting', gb_score)]
            best_model, best_name, best_score = max(scores, key=lambda x: x[2])
            
            with open(f'models/{best_name}_{horizon}.pkl', 'wb') as f:
                pickle.dump(best_model, f)
            
            print(f"      XGB: {xgb_score:.3f} | LGB: {lgb_score:.3f} | GB: {gb_score:.3f}")
            print(f"      âœ“ Best: {best_name} (RÂ²={best_score:.3f})")
        
        with open('models/scaler_ml.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('models/feature_names.json', 'w') as f:
            json.dump(selected_features, f)
        
        print("\n" + "="*70)
        print("âœ… DONE!")
        print(f"Features: {len(selected_features)}")
        EOF
    
    - name: Verify
      run: ls -lh models/*.pkl
    
    - name: Commit
      run: |
        git config user.email "bot@github.com"
        git config user.name "Bot"
        git add models/
        git diff --staged --quiet || git commit -m "ðŸ¤– $(date +'%Y-%m-%d')" && git push
