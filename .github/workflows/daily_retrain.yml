name: Daily Model Retraining

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  retrain:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GH_PAT }}
    
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install
      run: pip install pandas numpy scikit-learn xgboost lightgbm pymongo dnspython
    
    - name: Train with Feature Engineering
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        python - <<'EOF'
        import pandas as pd
        import numpy as np
        import pickle
        import json
        import os
        from sklearn.preprocessing import RobustScaler
        from sklearn.feature_selection import SelectKBest, mutual_info_regression
        import xgboost as xgb
        import lightgbm as lgb
        from sklearn.ensemble import GradientBoostingRegressor
        
        print("="*70)
        print("TRAINING WITH PROPER FEATURE ENGINEERING")
        print("="*70)
        
        # Load data
        print("\n1. Loading data...")
        try:
            from pymongo import MongoClient
            from pymongo.server_api import ServerApi
            client = MongoClient(os.environ['MONGODB_URI'], server_api=ServerApi('1'), serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            df = pd.DataFrame(list(client['aqi_feature_store']['aqi_features'].find({}, {"_id": 0})))
            client.close()
            print(f"   âœ“ MongoDB: {len(df)} rows")
        except:
            df = pd.read_csv('data/cleaned_aqi_data_v2.csv')
            print(f"   âœ“ CSV: {len(df)} rows")
        
        # Sort by time
        df['time'] = pd.to_datetime(df.get('time', df.get('timestamp')))
        df = df.sort_values('time').reset_index(drop=True)
        
        # Feature engineering (KEEP THIS - it improves RÂ²)
        print("\n2. Engineering features...")
        
        # Lag features
        for lag in [1, 3, 6, 12, 24]:
            if 'aqi' in df.columns:
                df[f'aqi_lag_{lag}h'] = df['aqi'].shift(lag)
        
        # Rolling features
        for window in [3, 6, 12, 24]:
            if 'aqi' in df.columns:
                df[f'aqi_ma_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).mean()
                df[f'aqi_std_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).std()
                df[f'aqi_min_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).min()
                df[f'aqi_max_{window}h'] = df['aqi'].rolling(window=window, min_periods=1).max()
        
        # Targets
        df['y24'] = df['aqi'].shift(-24)
        df['y48'] = df['aqi'].shift(-48)
        df['y72'] = df['aqi'].shift(-72)
        
        # Remove invalid rows
        df = df.dropna(subset=['y24','y48','y72'])
        
        # Get numeric features
        exclude = ['time', 'timestamp', 'y24', 'y48', 'y72', 
                   'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',
                   'season', 'weather_condition', 'day_of_week', 'day_of_month', 'is_weekend']
        
        feature_cols = [c for c in df.columns if c not in exclude]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        X = df[numeric_cols].fillna(df[numeric_cols].mean())
        y24, y48, y72 = df['y24'], df['y48'], df['y72']
        
        print(f"   âœ“ Features before selection: {len(X.columns)}")
        
        # Feature selection (IMPORTANT - keeps RÂ² high)
        print("\n3. Selecting best 30 features...")
        selector = SelectKBest(score_func=mutual_info_regression, k=min(30, len(X.columns)))
        selector.fit(X, y24)
        selected_features = X.columns[selector.get_support()].tolist()
        X = X[selected_features]
        
        print(f"   âœ“ Selected: {len(selected_features)} features")
        print(f"   Top 5: {selected_features[:5]}")
        
        # Split (time-series aware)
        split = int(len(X) * 0.8)
        X_train, X_test = X[:split], X[split:]
        y24_train, y24_test = y24[:split], y24[split:]
        y48_train, y48_test = y48[:split], y48[split:]
        y72_train, y72_test = y72[:split], y72[split:]
        
        print(f"   âœ“ Train: {len(X_train)}, Test: {len(X_test)}")
        
        # Scale
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        os.makedirs('models', exist_ok=True)
        
        # Train models (BETTER settings for good RÂ²)
        print("\n4. Training models...")
        
        for horizon, y_tr, y_te in [
            ('24h', y24_train, y24_test),
            ('48h', y48_train, y48_test),
            ('72h', y72_train, y72_test)
        ]:
            print(f"\n   {horizon}:")
            
            # XGBoost (balanced settings)
            xgb_model = xgb.XGBRegressor(
                n_estimators=100,       # Keep 100 for good performance
                max_depth=5,            # Restore depth
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1
            )
            xgb_model.fit(X_train_scaled, y_tr)
            xgb_score = xgb_model.score(X_test_scaled, y_te)
            
            # LightGBM
            lgb_model = lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                num_leaves=31,
                random_state=42,
                verbose=-1,
                n_jobs=-1
            )
            lgb_model.fit(X_train_scaled, y_tr)
            lgb_score = lgb_model.score(X_test_scaled, y_te)
            
            # Gradient Boosting (often best for this data)
            gb_model = GradientBoostingRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                random_state=42
            )
            gb_model.fit(X_train_scaled, y_tr)
            gb_score = gb_model.score(X_test_scaled, y_te)
            
            # Pick best
            scores = [
                (xgb_model, 'xgboost', xgb_score),
                (lgb_model, 'lightgbm', lgb_score),
                (gb_model, 'gradient_boosting', gb_score)
            ]
            best_model, best_name, best_score = max(scores, key=lambda x: x[2])
            
            # Save
            with open(f'models/{best_name}_{horizon}.pkl', 'wb') as f:
                pickle.dump(best_model, f)
            
            print(f"      XGBoost: RÂ²={xgb_score:.3f}")
            print(f"      LightGBM: RÂ²={lgb_score:.3f}")
            print(f"      GradBoost: RÂ²={gb_score:.3f}")
            print(f"      âœ“ Best: {best_name} (RÂ²={best_score:.3f})")
        
        # Save scaler and features
        with open('models/scaler_ml.pkl', 'wb') as f:
            pickle.dump(scaler, f)
        
        with open('models/feature_names.json', 'w') as f:
            json.dump(selected_features, f)
        
        with open('models/training_info.json', 'w') as f:
            json.dump({
                'n_features': len(selected_features),
                'n_samples': len(X),
                'features': selected_features
            }, f, indent=2)
        
        print("\n" + "="*70)
        print("âœ… TRAINING COMPLETE!")
        print("="*70)
        print(f"Features: {len(selected_features)}")
        print(f"Models saved in models/")
        EOF
    
    - name: Verify
      run: |
        echo "=== Models Created ==="
        ls -lh models/*.pkl
        echo ""
        echo "=== Training Info ==="
        cat models/training_info.json
    
    - name: Commit
      run: |
        git config user.email "actions@github.com"
        git config user.name "Actions Bot"
        git add models/
        git diff --staged --quiet || {
          git commit -m "ðŸ¤– Daily retrain $(date +'%Y-%m-%d %H:%M UTC')"
          git push
        }
