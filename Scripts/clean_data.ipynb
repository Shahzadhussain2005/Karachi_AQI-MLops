{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a31bc7-daff-4c94-9a9c-4eff1670d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AQI DATA CLEANING - OPTIMIZED VERSION\n",
      "======================================================================\n",
      "\n",
      "Starting OPTIMIZED AQI Data Cleaning\n",
      "\n",
      "\n",
      "Step 1: Loading data from data/cleaned_aqi_data_v2.csv\n",
      "✓ Loaded 4340 rows, 47 columns\n",
      "\n",
      "Step 2: Removing useless columns...\n",
      "✓ Removed 3 columns with 100% NaN: ['snwd', 'wpgt', 'tsun']\n",
      "✓ Removed redundant columns: ['aqi_color', 'time_of_day']\n",
      "✓ Columns: 47 → 42 (removed 5)\n",
      "\n",
      "Step 3: Handling missing values (smart strategy)...\n",
      "✓ Rows: 4340 → 4340 (lost 0)\n",
      "\n",
      "Step 4: Handling outliers...\n",
      "  pm10: Capped 17 extreme outliers\n",
      "  carbon_monoxide: Capped 2 extreme outliers\n",
      "  prcp: Capped 157 extreme outliers\n",
      "  coco: Capped 62 extreme outliers\n",
      "  aqi_lag_1h: Capped 62 extreme outliers\n",
      "  aqi_lag_3h: Capped 62 extreme outliers\n",
      "  aqi_lag_6h: Capped 62 extreme outliers\n",
      "  aqi_lag_12h: Capped 61 extreme outliers\n",
      "  aqi_lag_24h: Capped 61 extreme outliers\n",
      "  aqi_ma_6h: Capped 1 extreme outliers\n",
      "  aqi_std_6h: Capped 322 extreme outliers\n",
      "  aqi_std_12h: Capped 524 extreme outliers\n",
      "✓ Outliers handled\n",
      "\n",
      "Step 5: Fixing data types...\n",
      "  ✓ time → datetime\n",
      "✓ Data types fixed\n",
      "\n",
      "Step 6: Engineering features (minimal set)...\n",
      "  ✓ Created 3 AQI lag features\n",
      "  ✓ Created 3 PM2.5 lag features\n",
      "  ✓ Created 2 AQI rolling averages\n",
      "  ✓ Created cyclical hour features\n",
      "  ✓ Created weekday indicator\n",
      "  ✓ Created PM ratio\n",
      "✓ Total features: 46\n",
      "\n",
      "Step 7: Final validation...\n",
      "  ✓ No null values\n",
      "  ✓ No duplicates\n",
      "  ✓ No infinite values\n",
      "\n",
      "✓ Final dataset: 4340 rows × 46 columns\n",
      "\n",
      "Step 8: Saving to data/cleaned_aqi_data_v2.csv\n",
      "✓ Saved 4340 rows\n",
      "  File size: 1.54 MB\n",
      "\n",
      "======================================================================\n",
      "CLEANING COMPLETE - OPTIMIZED FOR ML\n",
      "======================================================================\n",
      "\n",
      "Input:  data/cleaned_aqi_data_v2.csv\n",
      "Output: data/cleaned_aqi_data_v2.csv\n",
      "\n",
      "Final: 4340 rows × 46 columns\n",
      "\n",
      "✅ Data cleaning optimized for better ML accuracy!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AQI Data Cleaning - OPTIMIZED FOR ML ACCURACY\n",
    "==============================================\n",
    "Improvements:\n",
    "- Removes 100% NaN columns (snwd, wpgt, tsun)\n",
    "- Better feature engineering\n",
    "- More data preservation\n",
    "- Optimized for model performance\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AQI DATA CLEANING - OPTIMIZED VERSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "INPUT_FILE = \"data/cleaned_aqi_data_v2.csv\"\n",
    "OUTPUT_FILE = \"data/cleaned_aqi_data_v2.csv\"\n",
    "REPORT_FILE = \"data/data_quality_report_v2.txt\"\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Data\n",
    "# ============================================================================\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load raw AQI data\"\"\"\n",
    "    print(f\"\\nStep 1: Loading data from {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"✓ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Remove Useless Columns\n",
    "# ============================================================================\n",
    "\n",
    "def remove_useless_columns(df):\n",
    "    \"\"\"Remove columns that are 100% NaN or have no predictive value\"\"\"\n",
    "    print(\"\\nStep 2: Removing useless columns...\")\n",
    "    \n",
    "    initial_cols = len(df.columns)\n",
    "    \n",
    "    # Remove columns with 100% missing values\n",
    "    nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "    if nan_cols:\n",
    "        df = df.drop(columns=nan_cols)\n",
    "        print(f\"✓ Removed {len(nan_cols)} columns with 100% NaN: {nan_cols}\")\n",
    "    \n",
    "    # Remove columns with >95% missing values\n",
    "    high_nan_cols = df.columns[df.isnull().sum() / len(df) > 0.95].tolist()\n",
    "    if high_nan_cols:\n",
    "        df = df.drop(columns=high_nan_cols)\n",
    "        print(f\"✓ Removed {len(high_nan_cols)} columns with >95% NaN: {high_nan_cols}\")\n",
    "    \n",
    "    # Remove non-predictive text columns (keep only useful categorical)\n",
    "    text_cols_to_remove = ['aqi_color', 'time_of_day']  # These are redundant\n",
    "    existing_to_remove = [col for col in text_cols_to_remove if col in df.columns]\n",
    "    if existing_to_remove:\n",
    "        df = df.drop(columns=existing_to_remove)\n",
    "        print(f\"✓ Removed redundant columns: {existing_to_remove}\")\n",
    "    \n",
    "    final_cols = len(df.columns)\n",
    "    print(f\"✓ Columns: {initial_cols} → {final_cols} (removed {initial_cols - final_cols})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Smart Missing Value Handling\n",
    "# ============================================================================\n",
    "\n",
    "def clean_missing_values_smart(df):\n",
    "    \"\"\"Handle missing values intelligently\"\"\"\n",
    "    print(\"\\nStep 3: Handling missing values (smart strategy)...\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # For pollutants: forward fill (pollution persists)\n",
    "    pollutant_cols = ['pm2_5', 'pm10', 'nitrogen_dioxide', 'ozone']\n",
    "    for col in pollutant_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].isnull().sum()\n",
    "            if before > 0:\n",
    "                # Forward fill with limit to avoid unrealistic propagation\n",
    "                df[col] = df[col].fillna(method='ffill', limit=3)\n",
    "                df[col] = df[col].fillna(method='bfill', limit=3)\n",
    "                # Fill remaining with median (more robust than mean)\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                after = df[col].isnull().sum()\n",
    "                print(f\"  {col}: {before} → {after} nulls\")\n",
    "    \n",
    "    # For weather: interpolate (smooth transitions)\n",
    "    weather_cols = ['temp', 'rhum', 'prcp', 'wdir', 'wspd', 'pres', 'cldc', 'coco']\n",
    "    for col in weather_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].isnull().sum()\n",
    "            if before > 0:\n",
    "                df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "                # Fill remaining with median\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                after = df[col].isnull().sum()\n",
    "                if before != after:\n",
    "                    print(f\"  {col}: {before} → {after} nulls\")\n",
    "    \n",
    "    # For AQI: forward fill (recent values are good predictors)\n",
    "    aqi_cols = ['aqi', 'aqi_pm25', 'aqi_pm10', 'aqi_o3', 'aqi_no2']\n",
    "    for col in aqi_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].isnull().sum()\n",
    "            if before > 0:\n",
    "                df[col] = df[col].fillna(method='ffill', limit=6)\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                after = df[col].isnull().sum()\n",
    "                if before != after:\n",
    "                    print(f\"  {col}: {before} → {after} nulls\")\n",
    "    \n",
    "    # Drop rows only if critical columns are still missing\n",
    "    critical_cols = ['time', 'aqi']\n",
    "    df = df.dropna(subset=[col for col in critical_cols if col in df.columns])\n",
    "    \n",
    "    final_rows = len(df)\n",
    "    print(f\"✓ Rows: {initial_rows} → {final_rows} (lost {initial_rows - final_rows})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Handle Outliers Carefully\n",
    "# ============================================================================\n",
    "\n",
    "def handle_outliers_smart(df):\n",
    "    \"\"\"Handle outliers without losing too much data\"\"\"\n",
    "    print(\"\\nStep 4: Handling outliers...\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Don't cap AQI-related columns (they can spike legitimately)\n",
    "    exclude_cols = ['aqi', 'aqi_pm25', 'aqi_pm10', 'aqi_o3', 'aqi_no2', \n",
    "                    'hour', 'day_of_week', 'month', 'is_weekend', 'day_of_month']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in exclude_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Use wider bounds (5 IQR instead of 3) to preserve more data\n",
    "            lower_bound = Q1 - 5 * IQR\n",
    "            upper_bound = Q3 + 5 * IQR\n",
    "            \n",
    "            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            \n",
    "            if outliers > 0:\n",
    "                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"  {col}: Capped {outliers} extreme outliers\")\n",
    "    \n",
    "    print(f\"✓ Outliers handled\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Fix Data Types\n",
    "# ============================================================================\n",
    "\n",
    "def fix_data_types(df):\n",
    "    \"\"\"Ensure correct data types\"\"\"\n",
    "    print(\"\\nStep 5: Fixing data types...\")\n",
    "    \n",
    "    if 'time' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        print(\"  ✓ time → datetime\")\n",
    "    \n",
    "    # Numeric columns\n",
    "    numeric_cols = ['pm2_5', 'pm10', 'nitrogen_dioxide', 'ozone', \n",
    "                    'temp', 'rhum', 'prcp', 'wdir', 'wspd', 'pres', 'cldc', 'coco']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Integer columns\n",
    "    int_cols = ['hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend',\n",
    "                'aqi', 'aqi_pm25', 'aqi_pm10', 'aqi_o3', 'aqi_no2']\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0).astype(int)\n",
    "    \n",
    "    print(\"✓ Data types fixed\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: MINIMAL Feature Engineering (Prevent Overfitting)\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_features_minimal(df):\n",
    "    \"\"\"Create ONLY essential features\"\"\"\n",
    "    print(\"\\nStep 6: Engineering features (minimal set)...\")\n",
    "    \n",
    "    if 'time' not in df.columns:\n",
    "        print(\"  ⚠ No 'time' column, skipping time features\")\n",
    "        return df\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # ONLY create the most important lag features\n",
    "    # Lags: 1h, 6h, 24h (recent, medium-term, daily pattern)\n",
    "    lag_hours = [1, 6, 24]\n",
    "    \n",
    "    if 'aqi' in df.columns:\n",
    "        for lag in lag_hours:\n",
    "            df[f'aqi_lag_{lag}h'] = df['aqi'].shift(lag)\n",
    "        print(f\"  ✓ Created {len(lag_hours)} AQI lag features\")\n",
    "    \n",
    "    if 'pm2_5' in df.columns:\n",
    "        for lag in lag_hours:\n",
    "            df[f'pm25_lag_{lag}h'] = df['pm2_5'].shift(lag)\n",
    "        print(f\"  ✓ Created {len(lag_hours)} PM2.5 lag features\")\n",
    "    \n",
    "    # ONLY create essential rolling features\n",
    "    # 6h and 24h moving averages (short and daily patterns)\n",
    "    if 'aqi' in df.columns:\n",
    "        df['aqi_ma_6h'] = df['aqi'].rolling(window=6, min_periods=1).mean()\n",
    "        df['aqi_ma_24h'] = df['aqi'].rolling(window=24, min_periods=1).mean()\n",
    "        print(\"  ✓ Created 2 AQI rolling averages\")\n",
    "    \n",
    "    # Cyclical time features (hour of day pattern)\n",
    "    if 'hour' in df.columns:\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        print(\"  ✓ Created cyclical hour features\")\n",
    "    \n",
    "    # Day of week pattern (weekday vs weekend)\n",
    "    if 'day_of_week' in df.columns:\n",
    "        df['is_weekday'] = (df['day_of_week'] < 5).astype(int)\n",
    "        print(\"  ✓ Created weekday indicator\")\n",
    "    \n",
    "    # Simple PM ratio (if both available)\n",
    "    if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "        df['pm_ratio'] = df['pm2_5'] / (df['pm10'] + 0.1)  # Avoid division by zero\n",
    "        print(\"  ✓ Created PM ratio\")\n",
    "    \n",
    "    # Fill NaN from lag/rolling operations (backward then forward)\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    print(f\"✓ Total features: {len(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Final Validation\n",
    "# ============================================================================\n",
    "\n",
    "def validate_cleaned_data(df):\n",
    "    \"\"\"Final validation\"\"\"\n",
    "    print(\"\\nStep 7: Final validation...\")\n",
    "    \n",
    "    # Check nulls\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"  ⚠ {null_count} null values remain\")\n",
    "        for col in df.columns:\n",
    "            nulls = df[col].isnull().sum()\n",
    "            if nulls > 0:\n",
    "                print(f\"    {col}: {nulls}\")\n",
    "    else:\n",
    "        print(\"  ✓ No null values\")\n",
    "    \n",
    "    # Check duplicates\n",
    "    dupes = df.duplicated().sum()\n",
    "    if dupes > 0:\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"  ✓ Removed {dupes} duplicates\")\n",
    "    else:\n",
    "        print(\"  ✓ No duplicates\")\n",
    "    \n",
    "    # Check infinite values\n",
    "    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    if inf_count > 0:\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        print(f\"  ✓ Replaced {inf_count} infinite values\")\n",
    "    else:\n",
    "        print(\"  ✓ No infinite values\")\n",
    "    \n",
    "    print(f\"\\n✓ Final dataset: {len(df)} rows × {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Save Data\n",
    "# ============================================================================\n",
    "\n",
    "def save_cleaned_data(df, output_file):\n",
    "    \"\"\"Save cleaned data\"\"\"\n",
    "    print(f\"\\nStep 8: Saving to {output_file}\")\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        print(f\"✓ Saved {len(df)} rows\")\n",
    "        print(f\"  File size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run optimized cleaning pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\nStarting OPTIMIZED AQI Data Cleaning\\n\")\n",
    "    \n",
    "    # Load\n",
    "    df = load_data(INPUT_FILE)\n",
    "    if df is None:\n",
    "        return False\n",
    "    \n",
    "    # Remove useless columns\n",
    "    df = remove_useless_columns(df)\n",
    "    \n",
    "    # Clean missing values\n",
    "    df = clean_missing_values_smart(df)\n",
    "    \n",
    "    # Handle outliers\n",
    "    df = handle_outliers_smart(df)\n",
    "    \n",
    "    # Fix types\n",
    "    df = fix_data_types(df)\n",
    "    \n",
    "    # Minimal feature engineering\n",
    "    df = engineer_features_minimal(df)\n",
    "    \n",
    "    # Validate\n",
    "    df = validate_cleaned_data(df)\n",
    "    \n",
    "    # Save\n",
    "    success = save_cleaned_data(df, OUTPUT_FILE)\n",
    "    \n",
    "    # Generate report\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLEANING COMPLETE - OPTIMIZED FOR ML\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nInput:  {INPUT_FILE}\")\n",
    "    print(f\"Output: {OUTPUT_FILE}\")\n",
    "    print(f\"\\nFinal: {len(df)} rows × {len(df.columns)} columns\")\n",
    "    \n",
    "    # Show key improvements\n",
    "    \n",
    "    \n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ Data cleaning optimized for better ML accuracy!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Pipeline failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bc1f4c-10cf-419a-80bb-f66ae8130d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_aqi_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d3e579-263e-498a-9501-4edc45804e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'pm2_5', 'pm10', 'nitrogen_dioxide', 'ozone', 'temp', 'rhum',\n",
       "       'prcp', 'wdir', 'wspd', 'pres', 'cldc', 'coco', 'aqi_pm25', 'aqi_pm10',\n",
       "       'aqi_o3', 'aqi_no2', 'aqi', 'dominant_pollutant', 'aqi_category',\n",
       "       'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend', 'season',\n",
       "       'aqi_lag_1h', 'aqi_lag_3h', 'aqi_lag_6h', 'aqi_lag_12h', 'aqi_lag_24h',\n",
       "       'pm25_lag_1h', 'pm25_lag_6h', 'pm25_lag_24h', 'aqi_ma_6h', 'aqi_std_6h',\n",
       "       'aqi_ma_12h', 'aqi_std_12h', 'aqi_ma_24h', 'aqi_std_24h', 'pm25_ma_6h',\n",
       "       'pm25_ma_24h', 'temp_rhum_interaction', 'pm_ratio', 'wind_pollution',\n",
       "       'hour_sin', 'hour_cos', 'is_weekday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68a347-42e4-47c7-a7e4-ea5b9c50ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index(['time', 'pm2_5', 'pm10', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide',\n",
    "       'carbon_monoxide', 'aqi_pm25', 'aqi_pm10', 'aqi', 'temp', 'rhum',\n",
    "       'prcp', 'snwd', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'cldc', 'coco',\n",
    "       'hour', 'day_of_week', 'day_of_month', 'month', 'day_of_year',\n",
    "       'is_weekend', 'time_of_day', 'season', 'aqi_lag_1h', 'aqi_lag_3h',\n",
    "       'aqi_lag_6h', 'aqi_lag_12h', 'aqi_lag_24h', 'pm25_lag_1h',\n",
    "       'pm25_lag_6h', 'pm25_lag_24h', 'aqi_ma_6h', 'aqi_std_6h', 'aqi_ma_12h',\n",
    "       'aqi_std_12h', 'aqi_ma_24h', 'aqi_std_24h', 'pm25_ma_6h', 'pm25_ma_24h',\n",
    "       'aqi_category', 'aqi_color'],\n",
    "      dtype='object')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
