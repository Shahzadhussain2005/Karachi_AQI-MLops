{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c3ac39-b3f9-4aa8-aa46-2f55bf8eb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "1. Loading data...\n",
      "\n",
      "Attempt 1/2: Connecting to MongoDB...\n",
      "âœ“ Connected!\n",
      "âœ“ Loaded 4322 records from MongoDB\n",
      "\n",
      "âœ“ Source: MONGODB\n",
      "âœ“ Records: 4322\n",
      "\n",
      "2. Engineering features...\n",
      "   Creating lag features...\n",
      "   Creating rolling features...\n",
      "   Creating difference features...\n",
      "âœ“ After engineering: 4322 records, 73 columns\n",
      "\n",
      "3. Preparing features...\n",
      "âœ“ Initial features: 61\n",
      "âœ“ After removing high-missing features: 61\n",
      "âœ“ Final dataset: 4250 records\n",
      "âœ“ Features: 61\n",
      "âœ“ Samples: 4250\n",
      "\n",
      "4. Feature selection...\n",
      "   Top 10 features:\n",
      "      day_of_year                   : 0.536\n",
      "      aqi_min_24h                   : 0.400\n",
      "      aqi_pm25                      : 0.336\n",
      "      aqi_min_12h                   : 0.335\n",
      "      pm2_5                         : 0.332\n",
      "      aqi                           : 0.323\n",
      "      aqi_min_6h                    : 0.313\n",
      "      aqi_ma_3h                     : 0.312\n",
      "      aqi_max_3h                    : 0.309\n",
      "      aqi_min_3h                    : 0.307\n",
      "\n",
      "âœ“ Selected 30 best features\n",
      "\n",
      "5. Splitting data (time-series aware)...\n",
      "âœ“ Train: 3400, Test: 850\n",
      "\n",
      "6. Setting up hyperparameter grids (optimized)...\n",
      "âœ“ 6 models configured for tuning\n",
      "âœ“ Using RandomizedSearchCV for faster training\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS WITH HYPERPARAMETER TUNING (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  This should take 5-10 minutes...\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 24h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV RÂ²: -0.128\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.003\n",
      "      Train RÂ²:    0.173\n",
      "      CV RÂ²:      -0.128\n",
      "      RMSE:        59.36\n",
      "      MAE:         29.79\n",
      "      MAPE:        26.24%\n",
      "      Acc Â±20:      50.0%\n",
      "      Acc Â±10:      22.1%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/ridge_24h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV RÂ²: -0.007\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.015\n",
      "      Train RÂ²:    0.161\n",
      "      CV RÂ²:      -0.007\n",
      "      RMSE:        58.99\n",
      "      MAE:         29.57\n",
      "      MAPE:        26.21%\n",
      "      Acc Â±20:      48.6%\n",
      "      Acc Â±10:      22.4%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/lasso_24h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'min_samples_split': 10, 'max_depth': None}\n",
      "   Best CV RÂ²: -0.683\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:    -0.081\n",
      "      Train RÂ²:    0.705\n",
      "      CV RÂ²:      -0.683\n",
      "      RMSE:        61.82\n",
      "      MAE:         32.63\n",
      "      MAPE:        29.11%\n",
      "      Acc Â±20:      47.3%\n",
      "      Acc Â±10:      22.7%\n",
      "      âš ï¸  OVERFITTING (gap: 0.786)\n",
      "      âœ“  Saved: models/random_forest_24h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.635\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.024\n",
      "      Train RÂ²:    0.279\n",
      "      CV RÂ²:      -0.635\n",
      "      RMSE:        58.75\n",
      "      MAE:         29.24\n",
      "      MAPE:        25.35%\n",
      "      Acc Â±20:      51.3%\n",
      "      Acc Â±10:      25.9%\n",
      "      âš ï¸  OVERFITTING (gap: 0.256)\n",
      "      âœ“  Saved: models/gradient_boosting_24h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.239\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.008\n",
      "      Train RÂ²:    0.269\n",
      "      CV RÂ²:      -0.239\n",
      "      RMSE:        59.22\n",
      "      MAE:         29.53\n",
      "      MAPE:        25.64%\n",
      "      Acc Â±20:      50.7%\n",
      "      Acc Â±10:      26.9%\n",
      "      âš ï¸  OVERFITTING (gap: 0.261)\n",
      "      âœ“  Saved: models/xgboost_24h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 31, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.080\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.003\n",
      "      Train RÂ²:    0.248\n",
      "      CV RÂ²:      -0.080\n",
      "      RMSE:        59.36\n",
      "      MAE:         29.35\n",
      "      MAPE:        25.21%\n",
      "      Acc Â±20:      51.1%\n",
      "      Acc Â±10:      26.7%\n",
      "      âš ï¸  OVERFITTING (gap: 0.245)\n",
      "      âœ“  Saved: models/lightgbm_24h.pkl\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 48h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV RÂ²: -0.964\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.013\n",
      "      Train RÂ²:    0.115\n",
      "      CV RÂ²:      -0.964\n",
      "      RMSE:        57.53\n",
      "      MAE:         29.34\n",
      "      MAPE:        27.82%\n",
      "      Acc Â±20:      46.7%\n",
      "      Acc Â±10:      25.3%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/ridge_48h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV RÂ²: -0.083\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.032\n",
      "      Train RÂ²:    0.100\n",
      "      CV RÂ²:      -0.083\n",
      "      RMSE:        56.96\n",
      "      MAE:         28.81\n",
      "      MAPE:        27.29%\n",
      "      Acc Â±20:      44.8%\n",
      "      Acc Â±10:      24.4%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/lasso_48h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 10}\n",
      "   Best CV RÂ²: -0.846\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:    -0.061\n",
      "      Train RÂ²:    0.697\n",
      "      CV RÂ²:      -0.846\n",
      "      RMSE:        59.64\n",
      "      MAE:         31.23\n",
      "      MAPE:        29.57%\n",
      "      Acc Â±20:      46.7%\n",
      "      Acc Â±10:      25.8%\n",
      "      âš ï¸  OVERFITTING (gap: 0.758)\n",
      "      âœ“  Saved: models/random_forest_48h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.246\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.010\n",
      "      Train RÂ²:    0.262\n",
      "      CV RÂ²:      -0.246\n",
      "      RMSE:        57.62\n",
      "      MAE:         28.88\n",
      "      MAPE:        26.18%\n",
      "      Acc Â±20:      48.4%\n",
      "      Acc Â±10:      26.5%\n",
      "      âš ï¸  OVERFITTING (gap: 0.252)\n",
      "      âœ“  Saved: models/gradient_boosting_48h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.117\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.034\n",
      "      Train RÂ²:    0.242\n",
      "      CV RÂ²:      -0.117\n",
      "      RMSE:        56.92\n",
      "      MAE:         27.85\n",
      "      MAPE:        25.04%\n",
      "      Acc Â±20:      49.9%\n",
      "      Acc Â±10:      26.4%\n",
      "      âš ï¸  OVERFITTING (gap: 0.208)\n",
      "      âœ“  Saved: models/xgboost_48h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 31, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.069\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.022\n",
      "      Train RÂ²:    0.218\n",
      "      CV RÂ²:      -0.069\n",
      "      RMSE:        57.25\n",
      "      MAE:         27.93\n",
      "      MAPE:        25.07%\n",
      "      Acc Â±20:      47.8%\n",
      "      Acc Â±10:      28.5%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/lightgbm_48h.pkl\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 72h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV RÂ²: -2.108\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.029\n",
      "      Train RÂ²:    0.097\n",
      "      CV RÂ²:      -2.108\n",
      "      RMSE:        55.30\n",
      "      MAE:         28.23\n",
      "      MAPE:        26.82%\n",
      "      Acc Â±20:      46.2%\n",
      "      Acc Â±10:      26.1%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/ridge_72h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV RÂ²: -0.367\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.040\n",
      "      Train RÂ²:    0.079\n",
      "      CV RÂ²:      -0.367\n",
      "      RMSE:        54.97\n",
      "      MAE:         28.17\n",
      "      MAPE:        27.23%\n",
      "      Acc Â±20:      46.0%\n",
      "      Acc Â±10:      24.9%\n",
      "      âš ï¸  VERY LOW RÂ² - Check data quality\n",
      "      âœ“  Saved: models/lasso_72h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': 10}\n",
      "   Best CV RÂ²: -0.180\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:    -0.030\n",
      "      Train RÂ²:    0.680\n",
      "      CV RÂ²:      -0.180\n",
      "      RMSE:        56.95\n",
      "      MAE:         29.80\n",
      "      MAPE:        28.87%\n",
      "      Acc Â±20:      50.1%\n",
      "      Acc Â±10:      23.9%\n",
      "      âš ï¸  OVERFITTING (gap: 0.710)\n",
      "      âœ“  Saved: models/random_forest_72h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.037\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:    -0.032\n",
      "      Train RÂ²:    0.287\n",
      "      CV RÂ²:      -0.037\n",
      "      RMSE:        57.01\n",
      "      MAE:         27.79\n",
      "      MAPE:        25.74%\n",
      "      Acc Â±20:      55.5%\n",
      "      Acc Â±10:      29.3%\n",
      "      âš ï¸  OVERFITTING (gap: 0.320)\n",
      "      âœ“  Saved: models/gradient_boosting_72h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 1.0, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.027\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.000\n",
      "      Train RÂ²:    0.253\n",
      "      CV RÂ²:      -0.027\n",
      "      RMSE:        56.11\n",
      "      MAE:         27.14\n",
      "      MAPE:        24.78%\n",
      "      Acc Â±20:      55.9%\n",
      "      Acc Â±10:      30.1%\n",
      "      âš ï¸  OVERFITTING (gap: 0.252)\n",
      "      âœ“  Saved: models/xgboost_72h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 31, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV RÂ²: -0.035\n",
      "\n",
      "   ğŸ“Š Results:\n",
      "      Test RÂ²:     0.019\n",
      "      Train RÂ²:    0.233\n",
      "      CV RÂ²:      -0.035\n",
      "      RMSE:        55.59\n",
      "      MAE:         26.52\n",
      "      MAPE:        24.01%\n",
      "      Acc Â±20:      56.5%\n",
      "      Acc Â±10:      29.8%\n",
      "      âš ï¸  OVERFITTING (gap: 0.214)\n",
      "      âœ“  Saved: models/lightgbm_72h.pkl\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Best 24h model: Gradient Boosting (RÂ² = 0.024)\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "   pm2_5                         : 0.3846\n",
      "   day_of_year                   : 0.1052\n",
      "   aqi_ma_12h                    : 0.0950\n",
      "   aqi                           : 0.0735\n",
      "   aqi_pm25                      : 0.0710\n",
      "   pres                          : 0.0522\n",
      "   aqi_max_12h                   : 0.0406\n",
      "   pm10                          : 0.0274\n",
      "   pm25_lag_24h                  : 0.0213\n",
      "   aqi_min_24h                   : 0.0144\n",
      "   aqi_max_3h                    : 0.0138\n",
      "   pm25_lag_1h                   : 0.0133\n",
      "   aqi_min_12h                   : 0.0124\n",
      "   pm25_ma_24h                   : 0.0124\n",
      "   aqi_ma_24h                    : 0.0076\n",
      "\n",
      "âœ“ Feature importance saved to: models/feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "24h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : RÂ²=  0.003  RMSE= 59.36  MAE= 29.79  AccÂ±20= 50.0%\n",
      "Lasso             : RÂ²=  0.015  RMSE= 58.99  MAE= 29.57  AccÂ±20= 48.6%\n",
      "Random Forest     : RÂ²= -0.081  RMSE= 61.82  MAE= 32.63  AccÂ±20= 47.3%\n",
      "Gradient Boosting : RÂ²=  0.024  RMSE= 58.75  MAE= 29.24  AccÂ±20= 51.3% â˜… BEST\n",
      "XGBoost           : RÂ²=  0.008  RMSE= 59.22  MAE= 29.53  AccÂ±20= 50.7%\n",
      "LightGBM          : RÂ²=  0.003  RMSE= 59.36  MAE= 29.35  AccÂ±20= 51.1%\n",
      "\n",
      "48h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : RÂ²=  0.013  RMSE= 57.53  MAE= 29.34  AccÂ±20= 46.7%\n",
      "Lasso             : RÂ²=  0.032  RMSE= 56.96  MAE= 28.81  AccÂ±20= 44.8%\n",
      "Random Forest     : RÂ²= -0.061  RMSE= 59.64  MAE= 31.23  AccÂ±20= 46.7%\n",
      "Gradient Boosting : RÂ²=  0.010  RMSE= 57.62  MAE= 28.88  AccÂ±20= 48.4%\n",
      "XGBoost           : RÂ²=  0.034  RMSE= 56.92  MAE= 27.85  AccÂ±20= 49.9% â˜… BEST\n",
      "LightGBM          : RÂ²=  0.022  RMSE= 57.25  MAE= 27.93  AccÂ±20= 47.8%\n",
      "\n",
      "72h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : RÂ²=  0.029  RMSE= 55.30  MAE= 28.23  AccÂ±20= 46.2%\n",
      "Lasso             : RÂ²=  0.040  RMSE= 54.97  MAE= 28.17  AccÂ±20= 46.0% â˜… BEST\n",
      "Random Forest     : RÂ²= -0.030  RMSE= 56.95  MAE= 29.80  AccÂ±20= 50.1%\n",
      "Gradient Boosting : RÂ²= -0.032  RMSE= 57.01  MAE= 27.79  AccÂ±20= 55.5%\n",
      "XGBoost           : RÂ²=  0.000  RMSE= 56.11  MAE= 27.14  AccÂ±20= 55.9%\n",
      "LightGBM          : RÂ²=  0.019  RMSE= 55.59  MAE= 26.52  AccÂ±20= 56.5%\n",
      "\n",
      "======================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Source: MONGODB\n",
      "ğŸ“ˆ Models: 6 ML models Ã— 3 horizons = 18 total\n",
      "ğŸ¯ All models hyperparameter-tuned with RandomizedSearchCV\n",
      "\n",
      "ğŸ“ Saved:\n",
      "  âœ“ models/*.pkl (tuned models)\n",
      "  âœ“ models/scaler_ml.pkl\n",
      "  âœ“ models/ml_tuned_results.json\n",
      "  âœ“ models/feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSTIC INFORMATION\n",
      "======================================================================\n",
      "\n",
      "Data Quality:\n",
      "   Total samples: 4250\n",
      "   Training samples: 3400\n",
      "   Test samples: 850\n",
      "   Features used: 30\n",
      "\n",
      "Target Statistics (24h ahead):\n",
      "   Mean: 102.25\n",
      "   Std: 56.03\n",
      "   Min: 29.00\n",
      "   Max: 500.00\n",
      "\n",
      "Best Model Performance:\n",
      "   Lasso (72h): RÂ² = 0.040\n",
      "\n",
      "âœ… Successfully trained 18/18 models\n",
      "\n",
      "âœ… Training pipeline completed successfully!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING (OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "\n",
    "def load_from_mongodb(uri, max_attempts=2):\n",
    "    \"\"\"Try MongoDB\"\"\"\n",
    "    from pymongo import MongoClient\n",
    "    from pymongo.server_api import ServerApi\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_attempts}: Connecting to MongoDB...\")\n",
    "            client = MongoClient(uri, server_api=ServerApi('1'),\n",
    "                               serverSelectionTimeoutMS=5000, connectTimeoutMS=5000)\n",
    "            client.admin.command('ping')\n",
    "            print(\"âœ“ Connected!\")\n",
    "            \n",
    "            db = client['aqi_feature_store']\n",
    "            collection = db['aqi_features']\n",
    "            data = pd.DataFrame(list(collection.find({}, {\"_id\": 0})))\n",
    "            client.close()\n",
    "            \n",
    "            print(f\"âœ“ Loaded {len(data)} records from MongoDB\")\n",
    "            return data, 'mongodb'\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {str(e)[:80]}\")\n",
    "    return None, None\n",
    "\n",
    "def load_from_csv(csv_path):\n",
    "    \"\"\"Load from CSV\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nLoading from CSV: {csv_path}\")\n",
    "        data = pd.read_csv(csv_path)\n",
    "        print(f\"âœ“ Loaded {len(data)} records\")\n",
    "        return data, 'csv'\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"\\n1. Loading data...\")\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://nawababbas08_db_user:2Ja4OGlDdKfG6EvZ@cluster0.jnxn95g.mongodb.net/?retryWrites=true&w=majority&tlsAllowInvalidCertificates=true\"\n",
    "CSV_PATH = \"data/cleaned_aqi_data_v2.csv\"\n",
    "\n",
    "data, source = load_from_mongodb(MONGO_URI, 2)\n",
    "if data is None:\n",
    "    print(\"\\nâš ï¸ MongoDB failed, using CSV...\")\n",
    "    data, source = load_from_csv(CSV_PATH)\n",
    "\n",
    "if data is None:\n",
    "    print(\"\\nâœ— ERROR: No data source available\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nâœ“ Source: {source.upper()}\")\n",
    "print(f\"âœ“ Records: {len(data)}\")\n",
    "\n",
    "\n",
    "print(\"\\n2. Engineering features...\")\n",
    "\n",
    "if 'time' in data.columns:\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    data = data.sort_values('time').reset_index(drop=True)\n",
    "elif 'timestamp' in data.columns:\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# More comprehensive lag features\n",
    "print(\"   Creating lag features...\")\n",
    "for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
    "    if 'aqi' in data.columns:\n",
    "        data[f'aqi_lag_{lag}h'] = data['aqi'].shift(lag)\n",
    "    if 'pm2_5' in data.columns:\n",
    "        data[f'pm25_lag_{lag}h'] = data['pm2_5'].shift(lag)\n",
    "\n",
    "# Rolling statistics (mean, std, min, max)\n",
    "print(\"   Creating rolling features...\")\n",
    "for window in [3, 6, 12, 24]:\n",
    "    if 'aqi' in data.columns:\n",
    "        data[f'aqi_ma_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).mean()\n",
    "        data[f'aqi_std_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).std()\n",
    "        data[f'aqi_min_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).min()\n",
    "        data[f'aqi_max_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "# Difference features (trend detection)\n",
    "print(\"   Creating difference features...\")\n",
    "if 'aqi' in data.columns:\n",
    "    data['aqi_diff_1h'] = data['aqi'].diff(1)\n",
    "    data['aqi_diff_3h'] = data['aqi'].diff(3)\n",
    "    data['aqi_diff_24h'] = data['aqi'].diff(24)\n",
    "\n",
    "# Cyclical features (better encoding)\n",
    "if 'hour' in data.columns:\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "\n",
    "if 'day_of_week' in data.columns:\n",
    "    data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "\n",
    "if 'month' in data.columns:\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Targets\n",
    "data['aqi_24h'] = data['aqi'].shift(-24)\n",
    "data['aqi_48h'] = data['aqi'].shift(-48)\n",
    "data['aqi_72h'] = data['aqi'].shift(-72)\n",
    "\n",
    "# Remove rows with all NaN\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "print(f\"âœ“ After engineering: {data.shape[0]} records, {data.shape[1]} columns\")\n",
    "\n",
    "\n",
    "print(\"\\n3. Preparing features...\")\n",
    "\n",
    "# Exclude target columns and categorical/string columns\n",
    "exclude_cols = ['time', 'timestamp', 'aqi_24h', 'aqi_48h', 'aqi_72h',\n",
    "                'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',\n",
    "                'season', 'weather_condition', 'day_of_week', 'day_of_month',\n",
    "                'is_weekend']\n",
    "\n",
    "# Get only numeric columns for features\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "numeric_cols = data[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = numeric_cols\n",
    "\n",
    "print(f\"âœ“ Initial features: {len(feature_cols)}\")\n",
    "\n",
    "# Remove features with too many missing values\n",
    "missing_threshold = 0.3\n",
    "for col in feature_cols[:]:\n",
    "    missing_pct = data[col].isnull().sum() / len(data)\n",
    "    if missing_pct > missing_threshold:\n",
    "        feature_cols.remove(col)\n",
    "        print(f\"   Removed {col} (missing: {missing_pct*100:.1f}%)\")\n",
    "\n",
    "print(f\"âœ“ After removing high-missing features: {len(feature_cols)}\")\n",
    "\n",
    "# Handle remaining missing values\n",
    "data[feature_cols] = data[feature_cols].fillna(data[feature_cols].mean())\n",
    "\n",
    "# Remove rows where target is missing\n",
    "data = data.dropna(subset=['aqi_24h', 'aqi_48h', 'aqi_72h'])\n",
    "\n",
    "print(f\"âœ“ Final dataset: {len(data)} records\")\n",
    "\n",
    "X = data[feature_cols]\n",
    "y_24h = data['aqi_24h']\n",
    "y_48h = data['aqi_48h']\n",
    "y_72h = data['aqi_72h']\n",
    "\n",
    "# Remove any remaining NaN\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "print(f\"âœ“ Features: {len(feature_cols)}\")\n",
    "print(f\"âœ“ Samples: {len(X)}\")\n",
    "\n",
    "\n",
    "print(\"\\n4. Feature selection...\")\n",
    "\n",
    "def select_best_features(X, y, k=30):\n",
    "    \"\"\"Select top K most important features\"\"\"\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Use mutual information for feature selection\n",
    "    selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    # Get scores\n",
    "    scores = selector.scores_\n",
    "    feature_scores = list(zip(X.columns, scores))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   Top 10 features:\")\n",
    "    for feat, score in feature_scores[:10]:\n",
    "        print(f\"      {feat:30s}: {score:.3f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Select features for 24h prediction\n",
    "selected_features = select_best_features(X, y_24h, k=min(30, len(X.columns)))\n",
    "X = X[selected_features]\n",
    "\n",
    "print(f\"\\nâœ“ Selected {len(selected_features)} best features\")\n",
    "\n",
    "\n",
    "print(\"\\n5. Splitting data (time-series aware)...\")\n",
    "\n",
    "# Use 80-20 split but maintain time order\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_24h_train, y_24h_test = y_24h[:split_idx], y_24h[split_idx:]\n",
    "y_48h_train, y_48h_test = y_48h[:split_idx], y_48h[split_idx:]\n",
    "y_72h_train, y_72h_test = y_72h[:split_idx], y_72h[split_idx:]\n",
    "\n",
    "# Use RobustScaler (better for outliers)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "with open('models/scaler_ml.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"âœ“ Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Define Model Hyperparameter Grids (OPTIMIZED!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. Setting up hyperparameter grids (optimized)...\")\n",
    "\n",
    "# MUCH SMALLER GRIDS - will run in 5-10 minutes instead of hours!\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1.0, 10.0]  # Reduced from 4 to 3 values\n",
    "        },\n",
    "        'n_iter': 3  # GridSearch will try all 3\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(max_iter=5000),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1.0]  # Reduced from 4 to 3 values\n",
    "        },\n",
    "        'n_iter': 3\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],  # Reduced from 3 to 2\n",
    "            'max_depth': [10, None],  # Reduced from 4 to 2\n",
    "            'min_samples_split': [5, 10]  # Reduced from 3 to 2\n",
    "        },\n",
    "        'n_iter': 8  # Will sample 8 random combinations instead of all 48\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],  # Reduced\n",
    "            'learning_rate': [0.05, 0.1],  # Reduced\n",
    "            'max_depth': [3, 5]  # Reduced\n",
    "        },\n",
    "        'n_iter': 8\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'n_iter': 10\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'num_leaves': [31, 63]\n",
    "        },\n",
    "        'n_iter': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ“ {len(param_grids)} models configured for tuning\")\n",
    "print(f\"âœ“ Using RandomizedSearchCV for faster training\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Accuracy within thresholds\n",
    "    acc_20 = np.sum(np.abs(y_true - y_pred) <= 20) / len(y_true) * 100\n",
    "    acc_10 = np.sum(np.abs(y_true - y_pred) <= 10) / len(y_true) * 100\n",
    "    acc_5 = np.sum(np.abs(y_true - y_pred) <= 5) / len(y_true) * 100\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'Acc20': acc_20,\n",
    "        'Acc10': acc_10,\n",
    "        'Acc5': acc_5\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Training with RandomizedSearchCV (FASTER!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODELS WITH HYPERPARAMETER TUNING (OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâ±ï¸  This should take 5-10 minutes...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Time series cross-validation - REDUCED to 2 splits for speed\n",
    "tscv = TimeSeriesSplit(n_splits=2)  # Changed from 3 to 2\n",
    "\n",
    "for horizon, y_train, y_test in [\n",
    "    ('24h', y_24h_train, y_24h_test),\n",
    "    ('48h', y_48h_train, y_48h_test),\n",
    "    ('72h', y_72h_train, y_72h_test)\n",
    "]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING FOR {horizon} AHEAD PREDICTION\")\n",
    "    print('='*70)\n",
    "    \n",
    "    results[horizon] = {}\n",
    "    \n",
    "    for name, config in param_grids.items():\n",
    "        try:\n",
    "            print(f\"\\n{name}...\")\n",
    "            print(f\"   Tuning hyperparameters...\")\n",
    "            \n",
    "            # Use RandomizedSearchCV for most models (faster than GridSearchCV)\n",
    "            if name in ['Ridge', 'Lasso']:\n",
    "                # GridSearch for simple models (fast anyway)\n",
    "                from sklearn.model_selection import GridSearchCV\n",
    "                search = GridSearchCV(\n",
    "                    estimator=config['model'],\n",
    "                    param_grid=config['params'],\n",
    "                    cv=tscv,\n",
    "                    scoring='r2',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "            else:\n",
    "                # RandomizedSearch for complex models (much faster)\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=config['model'],\n",
    "                    param_distributions=config['params'],\n",
    "                    n_iter=config['n_iter'],  # Only try N random combinations\n",
    "                    cv=tscv,\n",
    "                    scoring='r2',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0,\n",
    "                    random_state=42,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "            \n",
    "            # Fit search\n",
    "            search.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Best model\n",
    "            best_model = search.best_estimator_\n",
    "            \n",
    "            print(f\"   Best params: {search.best_params_}\")\n",
    "            print(f\"   Best CV RÂ²: {search.best_score_:.3f}\")\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred_train = best_model.predict(X_train_scaled)\n",
    "            y_pred_test = best_model.predict(X_test_scaled)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_metrics = evaluate(y_train, y_pred_train)\n",
    "            test_metrics = evaluate(y_test, y_pred_test)\n",
    "            \n",
    "            # Store results\n",
    "            results[horizon][name] = {\n",
    "                'test_R2': test_metrics['R2'],\n",
    "                'test_RMSE': test_metrics['RMSE'],\n",
    "                'test_MAE': test_metrics['MAE'],\n",
    "                'test_MAPE': test_metrics['MAPE'],\n",
    "                'test_Acc20': test_metrics['Acc20'],\n",
    "                'test_Acc10': test_metrics['Acc10'],\n",
    "                'test_Acc5': test_metrics['Acc5'],\n",
    "                'train_R2': train_metrics['R2'],\n",
    "                'cv_R2': search.best_score_,\n",
    "                'best_params': search.best_params_\n",
    "            }\n",
    "            \n",
    "            # Display metrics\n",
    "            print(f\"\\n   ğŸ“Š Results:\")\n",
    "            print(f\"      Test RÂ²:    {test_metrics['R2']:6.3f}\")\n",
    "            print(f\"      Train RÂ²:   {train_metrics['R2']:6.3f}\")\n",
    "            print(f\"      CV RÂ²:      {search.best_score_:6.3f}\")\n",
    "            print(f\"      RMSE:       {test_metrics['RMSE']:6.2f}\")\n",
    "            print(f\"      MAE:        {test_metrics['MAE']:6.2f}\")\n",
    "            print(f\"      MAPE:       {test_metrics['MAPE']:6.2f}%\")\n",
    "            print(f\"      Acc Â±20:    {test_metrics['Acc20']:6.1f}%\")\n",
    "            print(f\"      Acc Â±10:    {test_metrics['Acc10']:6.1f}%\")\n",
    "            \n",
    "            # Check for overfitting\n",
    "            overfit_gap = train_metrics['R2'] - test_metrics['R2']\n",
    "            if overfit_gap > 0.2:\n",
    "                print(f\"      âš ï¸  OVERFITTING (gap: {overfit_gap:.3f})\")\n",
    "            elif test_metrics['R2'] < 0:\n",
    "                print(f\"      âš ï¸  NEGATIVE RÂ² - Model performs worse than baseline!\")\n",
    "            elif test_metrics['R2'] < 0.1:\n",
    "                print(f\"      âš ï¸  VERY LOW RÂ² - Check data quality\")\n",
    "            else:\n",
    "                print(f\"      âœ“  Good performance!\")\n",
    "            \n",
    "            # Save model\n",
    "            model_path = f'models/{name.lower().replace(\" \", \"_\")}_{horizon}.pkl'\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(best_model, f)\n",
    "            print(f\"      âœ“  Saved: {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n      âŒ ERROR training {name} for {horizon}:\")\n",
    "            print(f\"      {str(e)}\")\n",
    "            print(f\"      Skipping this model and continuing...\")\n",
    "            \n",
    "            # Store error info\n",
    "            results[horizon][name] = {\n",
    "                'error': str(e),\n",
    "                'test_R2': -999,\n",
    "                'test_RMSE': -999,\n",
    "                'test_MAE': -999\n",
    "            }\n",
    "            continue\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Feature Importance Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze feature importance for best model\n",
    "best_24h = max(results['24h'].items(), key=lambda x: x[1]['test_R2'])\n",
    "print(f\"\\nBest 24h model: {best_24h[0]} (RÂ² = {best_24h[1]['test_R2']:.3f})\")\n",
    "\n",
    "# Save feature importance if available\n",
    "feature_importance_path = 'models/feature_importance.csv'\n",
    "try:\n",
    "    # Load the best model\n",
    "    model_name = best_24h[0].lower().replace(\" \", \"_\")\n",
    "    with open(f'models/{model_name}_24h.pkl', 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': selected_features,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        importance_df.to_csv(feature_importance_path, index=False)\n",
    "        \n",
    "        print(f\"\\nTop 15 Most Important Features:\")\n",
    "        for idx, row in importance_df.head(15).iterrows():\n",
    "            print(f\"   {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Feature importance saved to: {feature_importance_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Could not extract feature importance: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for horizon in results.keys():\n",
    "    print(f\"\\n{horizon} Ahead:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Filter out failed models\n",
    "    successful_models = {k: v for k, v in results[horizon].items() if v.get('test_R2', -999) != -999}\n",
    "    \n",
    "    if not successful_models:\n",
    "        print(f\"   âš ï¸  No models trained successfully for {horizon}\")\n",
    "        continue\n",
    "    \n",
    "    best = max(successful_models.items(), key=lambda x: x[1]['test_R2'])\n",
    "    \n",
    "    for name in results[horizon]:\n",
    "        m = results[horizon][name]\n",
    "        if m.get('test_R2', -999) == -999:\n",
    "            print(f\"{name:18s}: âŒ FAILED - {m.get('error', 'Unknown error')[:50]}\")\n",
    "        else:\n",
    "            marker = \" â˜… BEST\" if name == best[0] else \"\"\n",
    "            print(f\"{name:18s}: RÂ²={m['test_R2']:7.3f}  RMSE={m['test_RMSE']:6.2f}  \"\n",
    "                  f\"MAE={m['test_MAE']:6.2f}  AccÂ±20={m['test_Acc20']:5.1f}%{marker}\")\n",
    "\n",
    "# Save results\n",
    "with open('models/ml_tuned_results.json', 'w') as f:\n",
    "    # Convert to serializable format\n",
    "    results_serializable = {}\n",
    "    for horizon in results:\n",
    "        results_serializable[horizon] = {}\n",
    "        for model_name in results[horizon]:\n",
    "            results_serializable[horizon][model_name] = {\n",
    "                k: v for k, v in results[horizon][model_name].items() \n",
    "                if k != 'best_params'\n",
    "            }\n",
    "            results_serializable[horizon][model_name]['best_params_str'] = str(\n",
    "                results[horizon][model_name]['best_params']\n",
    "            )\n",
    "    \n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Source: {source.upper()}\")\n",
    "print(f\"ğŸ“ˆ Models: {len(param_grids)} ML models Ã— 3 horizons = {len(param_grids)*3} total\")\n",
    "print(f\"ğŸ¯ All models hyperparameter-tuned with RandomizedSearchCV\")\n",
    "print(\"\\nğŸ“ Saved:\")\n",
    "print(\"  âœ“ models/*.pkl (tuned models)\")\n",
    "print(\"  âœ“ models/scaler_ml.pkl\")\n",
    "print(\"  âœ“ models/ml_tuned_results.json\")\n",
    "print(\"  âœ“ models/feature_importance.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 11. Diagnostic Information\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"   Total samples: {len(data)}\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features used: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\nTarget Statistics (24h ahead):\")\n",
    "print(f\"   Mean: {y_24h.mean():.2f}\")\n",
    "print(f\"   Std: {y_24h.std():.2f}\")\n",
    "print(f\"   Min: {y_24h.min():.2f}\")\n",
    "print(f\"   Max: {y_24h.max():.2f}\")\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "all_results = [(h, n, m.get('test_R2', -999)) \n",
    "               for h in results \n",
    "               for n, m in results[h].items() \n",
    "               if m.get('test_R2', -999) != -999]\n",
    "\n",
    "if all_results:\n",
    "    best_overall = max(all_results, key=lambda x: x[2])\n",
    "    print(f\"   {best_overall[1]} ({best_overall[0]}): RÂ² = {best_overall[2]:.3f}\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  No models trained successfully\")\n",
    "\n",
    "\n",
    "# Check if at least some models trained successfully\n",
    "total_models = sum(1 for h in results for m in results[h] if results[h][m].get('test_R2', -999) != -999)\n",
    "total_attempted = sum(len(results[h]) for h in results)\n",
    "\n",
    "print(f\"\\nâœ… Successfully trained {total_models}/{total_attempted} models\")\n",
    "\n",
    "if total_models == 0:\n",
    "    print(\"\\nâš ï¸  WARNING: No models trained successfully!\")\n",
    "    print(\"Check the error messages above for details.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"\\nâœ… Training pipeline completed successfully!\")\n",
    "    import sys\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f534c14-2a05-4fdc-9c1d-02095ba4be61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
