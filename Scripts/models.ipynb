{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f39cbf-374f-429e-b653-22949c7b06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "1. Loading data...\n",
      "\n",
      "Attempt 1/2: Connecting to MongoDB...\n",
      "‚úì Connected!\n",
      "‚úì Loaded 4340 records from MongoDB\n",
      "\n",
      "‚úì Source: MONGODB\n",
      "‚úì Records: 4340\n",
      "\n",
      "2. Engineering features...\n",
      "   Creating lag features...\n",
      "   Creating rolling features...\n",
      "   Creating difference features...\n",
      "‚úì After engineering: 4340 records, 73 columns\n",
      "\n",
      "3. Preparing features...\n",
      "‚úì Initial features: 63\n",
      "‚úì After removing high-missing features: 63\n",
      "‚úì Final dataset: 4268 records\n",
      "‚úì Features: 63\n",
      "‚úì Samples: 4268\n",
      "\n",
      "4. Feature selection...\n",
      "   Top 10 features:\n",
      "      day_of_year                   : 0.537\n",
      "      aqi_min_24h                   : 0.392\n",
      "      aqi_min_12h                   : 0.338\n",
      "      aqi_pm25                      : 0.329\n",
      "      pm2_5                         : 0.326\n",
      "      aqi                           : 0.322\n",
      "      aqi_min_6h                    : 0.319\n",
      "      aqi_min_3h                    : 0.309\n",
      "      aqi_ma_3h                     : 0.305\n",
      "      aqi_max_3h                    : 0.303\n",
      "\n",
      "‚úì Selected 30 best features\n",
      "\n",
      "5. Splitting data (time-series aware)...\n",
      "‚úì Train: 3414, Test: 854\n",
      "\n",
      "6. Setting up hyperparameter grids (optimized)...\n",
      "‚úì 6 models configured for tuning\n",
      "‚úì Using RandomizedSearchCV for faster training\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS WITH HYPERPARAMETER TUNING (OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  This should take 5-10 minutes...\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 24h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV R¬≤: -0.106\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.007\n",
      "      Train R¬≤:    0.171\n",
      "      CV R¬≤:      -0.106\n",
      "      RMSE:        55.98\n",
      "      MAE:         28.73\n",
      "      MAPE:        25.48%\n",
      "      Acc ¬±20:      51.6%\n",
      "      Acc ¬±10:      23.1%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/ridge_24h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV R¬≤: -0.019\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.019\n",
      "      Train R¬≤:    0.160\n",
      "      CV R¬≤:      -0.019\n",
      "      RMSE:        55.64\n",
      "      MAE:         28.54\n",
      "      MAPE:        25.54%\n",
      "      Acc ¬±20:      49.3%\n",
      "      Acc ¬±10:      23.1%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/lasso_24h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 100, 'min_samples_split': 10, 'max_depth': 10}\n",
      "   Best CV R¬≤: -1.219\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:    -0.081\n",
      "      Train R¬≤:    0.627\n",
      "      CV R¬≤:      -1.219\n",
      "      RMSE:        58.41\n",
      "      MAE:         31.31\n",
      "      MAPE:        28.16%\n",
      "      Acc ¬±20:      48.2%\n",
      "      Acc ¬±10:      26.7%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.708)\n",
      "      ‚úì  Saved: models/random_forest_24h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -1.127\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.024\n",
      "      Train R¬≤:    0.276\n",
      "      CV R¬≤:      -1.127\n",
      "      RMSE:        55.51\n",
      "      MAE:         28.75\n",
      "      MAPE:        25.37%\n",
      "      Acc ¬±20:      49.8%\n",
      "      Acc ¬±10:      26.8%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.252)\n",
      "      ‚úì  Saved: models/gradient_boosting_24h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 1.0, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.558\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.019\n",
      "      Train R¬≤:    0.258\n",
      "      CV R¬≤:      -0.558\n",
      "      RMSE:        55.64\n",
      "      MAE:         28.96\n",
      "      MAPE:        25.60%\n",
      "      Acc ¬±20:      50.0%\n",
      "      Acc ¬±10:      26.6%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.238)\n",
      "      ‚úì  Saved: models/xgboost_24h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 31, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.361\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.004\n",
      "      Train R¬≤:    0.242\n",
      "      CV R¬≤:      -0.361\n",
      "      RMSE:        56.07\n",
      "      MAE:         28.93\n",
      "      MAPE:        25.43%\n",
      "      Acc ¬±20:      49.9%\n",
      "      Acc ¬±10:      27.3%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.238)\n",
      "      ‚úì  Saved: models/lightgbm_24h.pkl\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 48h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV R¬≤: -0.681\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.019\n",
      "      Train R¬≤:    0.119\n",
      "      CV R¬≤:      -0.681\n",
      "      RMSE:        58.76\n",
      "      MAE:         28.33\n",
      "      MAPE:        24.86%\n",
      "      Acc ¬±20:      50.7%\n",
      "      Acc ¬±10:      27.0%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/ridge_48h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV R¬≤: -0.090\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.032\n",
      "      Train R¬≤:    0.104\n",
      "      CV R¬≤:      -0.090\n",
      "      RMSE:        58.40\n",
      "      MAE:         27.86\n",
      "      MAPE:        24.30%\n",
      "      Acc ¬±20:      49.4%\n",
      "      Acc ¬±10:      27.4%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/lasso_48h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 10}\n",
      "   Best CV R¬≤: -2.005\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:    -0.009\n",
      "      Train R¬≤:    0.694\n",
      "      CV R¬≤:      -2.005\n",
      "      RMSE:        59.61\n",
      "      MAE:         30.12\n",
      "      MAPE:        27.32%\n",
      "      Acc ¬±20:      49.1%\n",
      "      Acc ¬±10:      27.5%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.704)\n",
      "      ‚úì  Saved: models/random_forest_48h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.706\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:    -0.001\n",
      "      Train R¬≤:    0.266\n",
      "      CV R¬≤:      -0.706\n",
      "      RMSE:        59.38\n",
      "      MAE:         28.03\n",
      "      MAPE:        23.19%\n",
      "      Acc ¬±20:      54.6%\n",
      "      Acc ¬±10:      27.3%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.268)\n",
      "      ‚úì  Saved: models/gradient_boosting_48h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 1.0, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.231\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.019\n",
      "      Train R¬≤:    0.234\n",
      "      CV R¬≤:      -0.231\n",
      "      RMSE:        58.78\n",
      "      MAE:         27.56\n",
      "      MAPE:        22.80%\n",
      "      Acc ¬±20:      54.3%\n",
      "      Acc ¬±10:      27.9%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.215)\n",
      "      ‚úì  Saved: models/xgboost_48h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 31, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.111\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.017\n",
      "      Train R¬≤:    0.222\n",
      "      CV R¬≤:      -0.111\n",
      "      RMSE:        58.84\n",
      "      MAE:         27.44\n",
      "      MAPE:        22.64%\n",
      "      Acc ¬±20:      55.0%\n",
      "      Acc ¬±10:      27.4%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.205)\n",
      "      ‚úì  Saved: models/lightgbm_48h.pkl\n",
      "\n",
      "======================================================================\n",
      "TRAINING FOR 72h AHEAD PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Ridge...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 10.0}\n",
      "   Best CV R¬≤: -1.668\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.015\n",
      "      Train R¬≤:    0.102\n",
      "      CV R¬≤:      -1.668\n",
      "      RMSE:        58.89\n",
      "      MAE:         28.04\n",
      "      MAPE:        24.28%\n",
      "      Acc ¬±20:      52.3%\n",
      "      Acc ¬±10:      28.5%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/ridge_72h.pkl\n",
      "\n",
      "Lasso...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'alpha': 1.0}\n",
      "   Best CV R¬≤: -0.387\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.033\n",
      "      Train R¬≤:    0.083\n",
      "      CV R¬≤:      -0.387\n",
      "      RMSE:        58.36\n",
      "      MAE:         27.41\n",
      "      MAPE:        23.87%\n",
      "      Acc ¬±20:      51.3%\n",
      "      Acc ¬±10:      27.5%\n",
      "      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\n",
      "      ‚úì  Saved: models/lasso_72h.pkl\n",
      "\n",
      "Random Forest...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 10}\n",
      "   Best CV R¬≤: -0.362\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:    -0.009\n",
      "      Train R¬≤:    0.685\n",
      "      CV R¬≤:      -0.362\n",
      "      RMSE:        59.61\n",
      "      MAE:         28.87\n",
      "      MAPE:        25.28%\n",
      "      Acc ¬±20:      54.1%\n",
      "      Acc ¬±10:      29.2%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.693)\n",
      "      ‚úì  Saved: models/random_forest_72h.pkl\n",
      "\n",
      "Gradient Boosting...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.135\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:    -0.007\n",
      "      Train R¬≤:    0.287\n",
      "      CV R¬≤:      -0.135\n",
      "      RMSE:        59.56\n",
      "      MAE:         27.00\n",
      "      MAPE:        22.12%\n",
      "      Acc ¬±20:      57.5%\n",
      "      Acc ¬±10:      31.0%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.294)\n",
      "      ‚úì  Saved: models/gradient_boosting_72h.pkl\n",
      "\n",
      "XGBoost...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.040\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.007\n",
      "      Train R¬≤:    0.265\n",
      "      CV R¬≤:      -0.040\n",
      "      RMSE:        59.15\n",
      "      MAE:         26.92\n",
      "      MAPE:        22.47%\n",
      "      Acc ¬±20:      58.2%\n",
      "      Acc ¬±10:      30.7%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.259)\n",
      "      ‚úì  Saved: models/xgboost_72h.pkl\n",
      "\n",
      "LightGBM...\n",
      "   Tuning hyperparameters...\n",
      "   Best params: {'num_leaves': 63, 'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.05}\n",
      "   Best CV R¬≤: -0.043\n",
      "\n",
      "   üìä Results:\n",
      "      Test R¬≤:     0.013\n",
      "      Train R¬≤:    0.388\n",
      "      CV R¬≤:      -0.043\n",
      "      RMSE:        58.96\n",
      "      MAE:         27.34\n",
      "      MAPE:        23.09%\n",
      "      Acc ¬±20:      56.9%\n",
      "      Acc ¬±10:      29.9%\n",
      "      ‚ö†Ô∏è  OVERFITTING (gap: 0.374)\n",
      "      ‚úì  Saved: models/lightgbm_72h.pkl\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Best 24h model: Gradient Boosting (R¬≤ = 0.024)\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "   pm2_5                         : 0.3887\n",
      "   day_of_year                   : 0.1138\n",
      "   aqi_ma_12h                    : 0.0947\n",
      "   aqi                           : 0.0718\n",
      "   aqi_pm25                      : 0.0665\n",
      "   pres                          : 0.0470\n",
      "   aqi_max_12h                   : 0.0409\n",
      "   pm10                          : 0.0249\n",
      "   aqi_ma_3h                     : 0.0207\n",
      "   pm25_lag_24h                  : 0.0190\n",
      "   pm25_lag_1h                   : 0.0175\n",
      "   aqi_ma_24h                    : 0.0121\n",
      "   aqi_std_24h                   : 0.0101\n",
      "   pm25_ma_24h                   : 0.0101\n",
      "   aqi_min_12h                   : 0.0080\n",
      "\n",
      "‚úì Feature importance saved to: models/feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "24h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : R¬≤=  0.007  RMSE= 55.98  MAE= 28.73  Acc¬±20= 51.6%\n",
      "Lasso             : R¬≤=  0.019  RMSE= 55.64  MAE= 28.54  Acc¬±20= 49.3%\n",
      "Random Forest     : R¬≤= -0.081  RMSE= 58.41  MAE= 31.31  Acc¬±20= 48.2%\n",
      "Gradient Boosting : R¬≤=  0.024  RMSE= 55.51  MAE= 28.75  Acc¬±20= 49.8% ‚òÖ BEST\n",
      "XGBoost           : R¬≤=  0.019  RMSE= 55.64  MAE= 28.96  Acc¬±20= 50.0%\n",
      "LightGBM          : R¬≤=  0.004  RMSE= 56.07  MAE= 28.93  Acc¬±20= 49.9%\n",
      "\n",
      "48h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : R¬≤=  0.019  RMSE= 58.76  MAE= 28.33  Acc¬±20= 50.7%\n",
      "Lasso             : R¬≤=  0.032  RMSE= 58.40  MAE= 27.86  Acc¬±20= 49.4% ‚òÖ BEST\n",
      "Random Forest     : R¬≤= -0.009  RMSE= 59.61  MAE= 30.12  Acc¬±20= 49.1%\n",
      "Gradient Boosting : R¬≤= -0.001  RMSE= 59.38  MAE= 28.03  Acc¬±20= 54.6%\n",
      "XGBoost           : R¬≤=  0.019  RMSE= 58.78  MAE= 27.56  Acc¬±20= 54.3%\n",
      "LightGBM          : R¬≤=  0.017  RMSE= 58.84  MAE= 27.44  Acc¬±20= 55.0%\n",
      "\n",
      "72h Ahead:\n",
      "----------------------------------------------------------------------\n",
      "Ridge             : R¬≤=  0.015  RMSE= 58.89  MAE= 28.04  Acc¬±20= 52.3%\n",
      "Lasso             : R¬≤=  0.033  RMSE= 58.36  MAE= 27.41  Acc¬±20= 51.3% ‚òÖ BEST\n",
      "Random Forest     : R¬≤= -0.009  RMSE= 59.61  MAE= 28.87  Acc¬±20= 54.1%\n",
      "Gradient Boosting : R¬≤= -0.007  RMSE= 59.56  MAE= 27.00  Acc¬±20= 57.5%\n",
      "XGBoost           : R¬≤=  0.007  RMSE= 59.15  MAE= 26.92  Acc¬±20= 58.2%\n",
      "LightGBM          : R¬≤=  0.013  RMSE= 58.96  MAE= 27.34  Acc¬±20= 56.9%\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Source: MONGODB\n",
      "üìà Models: 6 ML models √ó 3 horizons = 18 total\n",
      "üéØ All models hyperparameter-tuned with RandomizedSearchCV\n",
      "\n",
      "üìÅ Saved:\n",
      "  ‚úì models/*.pkl (tuned models)\n",
      "  ‚úì models/scaler_ml.pkl\n",
      "  ‚úì models/ml_tuned_results.json\n",
      "  ‚úì models/feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSTIC INFORMATION\n",
      "======================================================================\n",
      "\n",
      "Data Quality:\n",
      "   Total samples: 4268\n",
      "   Training samples: 3414\n",
      "   Test samples: 854\n",
      "   Features used: 30\n",
      "\n",
      "Target Statistics (24h ahead):\n",
      "   Mean: 101.86\n",
      "   Std: 55.66\n",
      "   Min: 29.00\n",
      "   Max: 500.00\n",
      "\n",
      "Best Model Performance:\n",
      "   Lasso (72h): R¬≤ = 0.033\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZATION NOTES\n",
      "======================================================================\n",
      "‚úì Using RandomizedSearchCV instead of GridSearchCV\n",
      "‚úì Reduced hyperparameter grid sizes\n",
      "‚úì Training all 3 horizons (24h, 48h, 72h)\n",
      "‚úì Reduced CV splits from 3 to 2\n",
      "‚úì Expected runtime: 10-20 minutes (vs 1+ hour previously)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING (OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Load Data\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_mongodb(uri, max_attempts=2):\n",
    "    \"\"\"Try MongoDB\"\"\"\n",
    "    from pymongo import MongoClient\n",
    "    from pymongo.server_api import ServerApi\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_attempts}: Connecting to MongoDB...\")\n",
    "            client = MongoClient(uri, server_api=ServerApi('1'),\n",
    "                               serverSelectionTimeoutMS=5000, connectTimeoutMS=5000)\n",
    "            client.admin.command('ping')\n",
    "            print(\"‚úì Connected!\")\n",
    "            \n",
    "            db = client['aqi_feature_store']\n",
    "            collection = db['aqi_features']\n",
    "            data = pd.DataFrame(list(collection.find({}, {\"_id\": 0})))\n",
    "            client.close()\n",
    "            \n",
    "            print(f\"‚úì Loaded {len(data)} records from MongoDB\")\n",
    "            return data, 'mongodb'\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed: {str(e)[:80]}\")\n",
    "    return None, None\n",
    "\n",
    "def load_from_csv(csv_path):\n",
    "    \"\"\"Load from CSV\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nLoading from CSV: {csv_path}\")\n",
    "        data = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì Loaded {len(data)} records\")\n",
    "        return data, 'csv'\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"\\n1. Loading data...\")\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://nawababbas08_db_user:2Ja4OGlDdKfG6EvZ@cluster0.jnxn95g.mongodb.net/?retryWrites=true&w=majority&tlsAllowInvalidCertificates=true\"\n",
    "CSV_PATH = \"data/cleaned_aqi_data_v2.csv\"\n",
    "\n",
    "data, source = load_from_mongodb(MONGO_URI, 2)\n",
    "if data is None:\n",
    "    print(\"\\n‚ö†Ô∏è MongoDB failed, using CSV...\")\n",
    "    data, source = load_from_csv(CSV_PATH)\n",
    "\n",
    "if data is None:\n",
    "    print(\"\\n‚úó ERROR: No data source available\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\n‚úì Source: {source.upper()}\")\n",
    "print(f\"‚úì Records: {len(data)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Enhanced Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2. Engineering features...\")\n",
    "\n",
    "if 'time' in data.columns:\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    data = data.sort_values('time').reset_index(drop=True)\n",
    "elif 'timestamp' in data.columns:\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# More comprehensive lag features\n",
    "print(\"   Creating lag features...\")\n",
    "for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
    "    if 'aqi' in data.columns:\n",
    "        data[f'aqi_lag_{lag}h'] = data['aqi'].shift(lag)\n",
    "    if 'pm2_5' in data.columns:\n",
    "        data[f'pm25_lag_{lag}h'] = data['pm2_5'].shift(lag)\n",
    "\n",
    "# Rolling statistics (mean, std, min, max)\n",
    "print(\"   Creating rolling features...\")\n",
    "for window in [3, 6, 12, 24]:\n",
    "    if 'aqi' in data.columns:\n",
    "        data[f'aqi_ma_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).mean()\n",
    "        data[f'aqi_std_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).std()\n",
    "        data[f'aqi_min_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).min()\n",
    "        data[f'aqi_max_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "# Difference features (trend detection)\n",
    "print(\"   Creating difference features...\")\n",
    "if 'aqi' in data.columns:\n",
    "    data['aqi_diff_1h'] = data['aqi'].diff(1)\n",
    "    data['aqi_diff_3h'] = data['aqi'].diff(3)\n",
    "    data['aqi_diff_24h'] = data['aqi'].diff(24)\n",
    "\n",
    "# Cyclical features (better encoding)\n",
    "if 'hour' in data.columns:\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "\n",
    "if 'day_of_week' in data.columns:\n",
    "    data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "\n",
    "if 'month' in data.columns:\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Targets\n",
    "data['aqi_24h'] = data['aqi'].shift(-24)\n",
    "data['aqi_48h'] = data['aqi'].shift(-48)\n",
    "data['aqi_72h'] = data['aqi'].shift(-72)\n",
    "\n",
    "# Remove rows with all NaN\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "print(f\"‚úì After engineering: {data.shape[0]} records, {data.shape[1]} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Prepare Data with Better Filtering\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3. Preparing features...\")\n",
    "\n",
    "# Exclude target columns and categorical/string columns\n",
    "exclude_cols = ['time', 'timestamp', 'aqi_24h', 'aqi_48h', 'aqi_72h',\n",
    "                'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',\n",
    "                'season', 'weather_condition', 'day_of_week', 'day_of_month',\n",
    "                'is_weekend']\n",
    "\n",
    "# Get only numeric columns for features\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "numeric_cols = data[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = numeric_cols\n",
    "\n",
    "print(f\"‚úì Initial features: {len(feature_cols)}\")\n",
    "\n",
    "# Remove features with too many missing values\n",
    "missing_threshold = 0.3\n",
    "for col in feature_cols[:]:\n",
    "    missing_pct = data[col].isnull().sum() / len(data)\n",
    "    if missing_pct > missing_threshold:\n",
    "        feature_cols.remove(col)\n",
    "        print(f\"   Removed {col} (missing: {missing_pct*100:.1f}%)\")\n",
    "\n",
    "print(f\"‚úì After removing high-missing features: {len(feature_cols)}\")\n",
    "\n",
    "# Handle remaining missing values\n",
    "data[feature_cols] = data[feature_cols].fillna(data[feature_cols].mean())\n",
    "\n",
    "# Remove rows where target is missing\n",
    "data = data.dropna(subset=['aqi_24h', 'aqi_48h', 'aqi_72h'])\n",
    "\n",
    "print(f\"‚úì Final dataset: {len(data)} records\")\n",
    "\n",
    "X = data[feature_cols]\n",
    "y_24h = data['aqi_24h']\n",
    "y_48h = data['aqi_48h']\n",
    "y_72h = data['aqi_72h']\n",
    "\n",
    "# Remove any remaining NaN\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "print(f\"‚úì Features: {len(feature_cols)}\")\n",
    "print(f\"‚úì Samples: {len(X)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Feature Selection\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. Feature selection...\")\n",
    "\n",
    "def select_best_features(X, y, k=30):\n",
    "    \"\"\"Select top K most important features\"\"\"\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Use mutual information for feature selection\n",
    "    selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    # Get scores\n",
    "    scores = selector.scores_\n",
    "    feature_scores = list(zip(X.columns, scores))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   Top 10 features:\")\n",
    "    for feat, score in feature_scores[:10]:\n",
    "        print(f\"      {feat:30s}: {score:.3f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Select features for 24h prediction\n",
    "selected_features = select_best_features(X, y_24h, k=min(30, len(X.columns)))\n",
    "X = X[selected_features]\n",
    "\n",
    "print(f\"\\n‚úì Selected {len(selected_features)} best features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Time Series Split (Better for Time Series!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5. Splitting data (time-series aware)...\")\n",
    "\n",
    "# Use 80-20 split but maintain time order\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_24h_train, y_24h_test = y_24h[:split_idx], y_24h[split_idx:]\n",
    "y_48h_train, y_48h_test = y_48h[:split_idx], y_48h[split_idx:]\n",
    "y_72h_train, y_72h_test = y_72h[:split_idx], y_72h[split_idx:]\n",
    "\n",
    "# Use RobustScaler (better for outliers)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "with open('models/scaler_ml.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"‚úì Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Define Model Hyperparameter Grids (OPTIMIZED!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. Setting up hyperparameter grids (optimized)...\")\n",
    "\n",
    "# MUCH SMALLER GRIDS - will run in 5-10 minutes instead of hours!\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1.0, 10.0]  # Reduced from 4 to 3 values\n",
    "        },\n",
    "        'n_iter': 3  # GridSearch will try all 3\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(max_iter=5000),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1.0]  # Reduced from 4 to 3 values\n",
    "        },\n",
    "        'n_iter': 3\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],  # Reduced from 3 to 2\n",
    "            'max_depth': [10, None],  # Reduced from 4 to 2\n",
    "            'min_samples_split': [5, 10]  # Reduced from 3 to 2\n",
    "        },\n",
    "        'n_iter': 8  # Will sample 8 random combinations instead of all 48\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],  # Reduced\n",
    "            'learning_rate': [0.05, 0.1],  # Reduced\n",
    "            'max_depth': [3, 5]  # Reduced\n",
    "        },\n",
    "        'n_iter': 8\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'n_iter': 10\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'num_leaves': [31, 63]\n",
    "        },\n",
    "        'n_iter': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úì {len(param_grids)} models configured for tuning\")\n",
    "print(f\"‚úì Using RandomizedSearchCV for faster training\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Accuracy within thresholds\n",
    "    acc_20 = np.sum(np.abs(y_true - y_pred) <= 20) / len(y_true) * 100\n",
    "    acc_10 = np.sum(np.abs(y_true - y_pred) <= 10) / len(y_true) * 100\n",
    "    acc_5 = np.sum(np.abs(y_true - y_pred) <= 5) / len(y_true) * 100\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'Acc20': acc_20,\n",
    "        'Acc10': acc_10,\n",
    "        'Acc5': acc_5\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Training with RandomizedSearchCV (FASTER!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODELS WITH HYPERPARAMETER TUNING (OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚è±Ô∏è  This should take 5-10 minutes...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Time series cross-validation - REDUCED to 2 splits for speed\n",
    "tscv = TimeSeriesSplit(n_splits=2)  # Changed from 3 to 2\n",
    "\n",
    "for horizon, y_train, y_test in [\n",
    "    ('24h', y_24h_train, y_24h_test),\n",
    "    ('48h', y_48h_train, y_48h_test),\n",
    "    ('72h', y_72h_train, y_72h_test)\n",
    "]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING FOR {horizon} AHEAD PREDICTION\")\n",
    "    print('='*70)\n",
    "    \n",
    "    results[horizon] = {}\n",
    "    \n",
    "    for name, config in param_grids.items():\n",
    "        print(f\"\\n{name}...\")\n",
    "        print(f\"   Tuning hyperparameters...\")\n",
    "        \n",
    "        # Use RandomizedSearchCV for most models (faster than GridSearchCV)\n",
    "        if name in ['Ridge', 'Lasso']:\n",
    "            # GridSearch for simple models (fast anyway)\n",
    "            from sklearn.model_selection import GridSearchCV\n",
    "            search = GridSearchCV(\n",
    "                estimator=config['model'],\n",
    "                param_grid=config['params'],\n",
    "                cv=tscv,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "        else:\n",
    "            # RandomizedSearch for complex models (much faster)\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=config['model'],\n",
    "                param_distributions=config['params'],\n",
    "                n_iter=config['n_iter'],  # Only try N random combinations\n",
    "                cv=tscv,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        # Fit search\n",
    "        search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Best model\n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        print(f\"   Best params: {search.best_params_}\")\n",
    "        print(f\"   Best CV R¬≤: {search.best_score_:.3f}\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = best_model.predict(X_train_scaled)\n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_metrics = evaluate(y_train, y_pred_train)\n",
    "        test_metrics = evaluate(y_test, y_pred_test)\n",
    "        \n",
    "        # Store results\n",
    "        results[horizon][name] = {\n",
    "            'test_R2': test_metrics['R2'],\n",
    "            'test_RMSE': test_metrics['RMSE'],\n",
    "            'test_MAE': test_metrics['MAE'],\n",
    "            'test_MAPE': test_metrics['MAPE'],\n",
    "            'test_Acc20': test_metrics['Acc20'],\n",
    "            'test_Acc10': test_metrics['Acc10'],\n",
    "            'test_Acc5': test_metrics['Acc5'],\n",
    "            'train_R2': train_metrics['R2'],\n",
    "            'cv_R2': search.best_score_,\n",
    "            'best_params': search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Display metrics\n",
    "        print(f\"\\n   üìä Results:\")\n",
    "        print(f\"      Test R¬≤:    {test_metrics['R2']:6.3f}\")\n",
    "        print(f\"      Train R¬≤:   {train_metrics['R2']:6.3f}\")\n",
    "        print(f\"      CV R¬≤:      {search.best_score_:6.3f}\")\n",
    "        print(f\"      RMSE:       {test_metrics['RMSE']:6.2f}\")\n",
    "        print(f\"      MAE:        {test_metrics['MAE']:6.2f}\")\n",
    "        print(f\"      MAPE:       {test_metrics['MAPE']:6.2f}%\")\n",
    "        print(f\"      Acc ¬±20:    {test_metrics['Acc20']:6.1f}%\")\n",
    "        print(f\"      Acc ¬±10:    {test_metrics['Acc10']:6.1f}%\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        overfit_gap = train_metrics['R2'] - test_metrics['R2']\n",
    "        if overfit_gap > 0.2:\n",
    "            print(f\"      ‚ö†Ô∏è  OVERFITTING (gap: {overfit_gap:.3f})\")\n",
    "        elif test_metrics['R2'] < 0:\n",
    "            print(f\"      ‚ö†Ô∏è  NEGATIVE R¬≤ - Model performs worse than baseline!\")\n",
    "        elif test_metrics['R2'] < 0.1:\n",
    "            print(f\"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\")\n",
    "        else:\n",
    "            print(f\"      ‚úì  Good performance!\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = f'models/{name.lower().replace(\" \", \"_\")}_{horizon}.pkl'\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        print(f\"      ‚úì  Saved: {model_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Feature Importance Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze feature importance for best model\n",
    "best_24h = max(results['24h'].items(), key=lambda x: x[1]['test_R2'])\n",
    "print(f\"\\nBest 24h model: {best_24h[0]} (R¬≤ = {best_24h[1]['test_R2']:.3f})\")\n",
    "\n",
    "# Save feature importance if available\n",
    "feature_importance_path = 'models/feature_importance.csv'\n",
    "try:\n",
    "    # Load the best model\n",
    "    model_name = best_24h[0].lower().replace(\" \", \"_\")\n",
    "    with open(f'models/{model_name}_24h.pkl', 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': selected_features,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        importance_df.to_csv(feature_importance_path, index=False)\n",
    "        \n",
    "        print(f\"\\nTop 15 Most Important Features:\")\n",
    "        for idx, row in importance_df.head(15).iterrows():\n",
    "            print(f\"   {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Feature importance saved to: {feature_importance_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not extract feature importance: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for horizon in results.keys():\n",
    "    print(f\"\\n{horizon} Ahead:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    best = max(results[horizon].items(), key=lambda x: x[1]['test_R2'])\n",
    "    \n",
    "    for name in results[horizon]:\n",
    "        m = results[horizon][name]\n",
    "        marker = \" ‚òÖ BEST\" if name == best[0] else \"\"\n",
    "        print(f\"{name:18s}: R¬≤={m['test_R2']:7.3f}  RMSE={m['test_RMSE']:6.2f}  \"\n",
    "              f\"MAE={m['test_MAE']:6.2f}  Acc¬±20={m['test_Acc20']:5.1f}%{marker}\")\n",
    "\n",
    "# Save results\n",
    "with open('models/ml_tuned_results.json', 'w') as f:\n",
    "    # Convert to serializable format\n",
    "    results_serializable = {}\n",
    "    for horizon in results:\n",
    "        results_serializable[horizon] = {}\n",
    "        for model_name in results[horizon]:\n",
    "            results_serializable[horizon][model_name] = {\n",
    "                k: v for k, v in results[horizon][model_name].items() \n",
    "                if k != 'best_params'\n",
    "            }\n",
    "            results_serializable[horizon][model_name]['best_params_str'] = str(\n",
    "                results[horizon][model_name]['best_params']\n",
    "            )\n",
    "    \n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Source: {source.upper()}\")\n",
    "print(f\"üìà Models: {len(param_grids)} ML models √ó 3 horizons = {len(param_grids)*3} total\")\n",
    "print(f\"üéØ All models hyperparameter-tuned with RandomizedSearchCV\")\n",
    "print(\"\\nüìÅ Saved:\")\n",
    "print(\"  ‚úì models/*.pkl (tuned models)\")\n",
    "print(\"  ‚úì models/scaler_ml.pkl\")\n",
    "print(\"  ‚úì models/ml_tuned_results.json\")\n",
    "print(\"  ‚úì models/feature_importance.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 11. Diagnostic Information\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"   Total samples: {len(data)}\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features used: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\nTarget Statistics (24h ahead):\")\n",
    "print(f\"   Mean: {y_24h.mean():.2f}\")\n",
    "print(f\"   Std: {y_24h.std():.2f}\")\n",
    "print(f\"   Min: {y_24h.min():.2f}\")\n",
    "print(f\"   Max: {y_24h.max():.2f}\")\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "best_overall = max(\n",
    "    [(h, n, m['test_R2']) for h in results for n, m in results[h].items()],\n",
    "    key=lambda x: x[2]\n",
    ")\n",
    "print(f\"   {best_overall[1]} ({best_overall[0]}): R¬≤ = {best_overall[2]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd180c-65ad-4834-b031-1aa3e4b85b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"5dd64233-85cd-44fc-b98e-e64e6d0dacff\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"======================================================================\\n\",\n",
    "      \"ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"1. Loading data...\\n\",\n",
    "      \"\\n\",\n",
    "      \"Attempt 1/2: Connecting to MongoDB...\\n\",\n",
    "      \"‚úì Connected!\\n\",\n",
    "      \"‚úì Loaded 4340 records from MongoDB\\n\",\n",
    "      \"\\n\",\n",
    "      \"‚úì Source: MONGODB\\n\",\n",
    "      \"‚úì Records: 4340\\n\",\n",
    "      \"\\n\",\n",
    "      \"2. Engineering features...\\n\",\n",
    "      \"   Creating lag features...\\n\",\n",
    "      \"   Creating rolling features...\\n\",\n",
    "      \"   Creating difference features...\\n\",\n",
    "      \"‚úì After engineering: 4340 records, 73 columns\\n\",\n",
    "      \"\\n\",\n",
    "      \"3. Preparing features...\\n\",\n",
    "      \"‚úì Initial features: 63\\n\",\n",
    "      \"‚úì After removing high-missing features: 63\\n\",\n",
    "      \"‚úì Final dataset: 4268 records\\n\",\n",
    "      \"‚úì Features: 63\\n\",\n",
    "      \"‚úì Samples: 4268\\n\",\n",
    "      \"\\n\",\n",
    "      \"4. Feature selection...\\n\",\n",
    "      \"   Top 10 features:\\n\",\n",
    "      \"      day_of_year                   : 0.526\\n\",\n",
    "      \"      aqi_min_24h                   : 0.390\\n\",\n",
    "      \"      aqi_min_12h                   : 0.346\\n\",\n",
    "      \"      aqi_pm25                      : 0.325\\n\",\n",
    "      \"      pm2_5                         : 0.323\\n\",\n",
    "      \"      aqi_min_6h                    : 0.322\\n\",\n",
    "      \"      aqi                           : 0.314\\n\",\n",
    "      \"      aqi_min_3h                    : 0.314\\n\",\n",
    "      \"      aqi_ma_3h                     : 0.307\\n\",\n",
    "      \"      aqi_max_3h                    : 0.301\\n\",\n",
    "      \"\\n\",\n",
    "      \"‚úì Selected 30 best features\\n\",\n",
    "      \"\\n\",\n",
    "      \"5. Splitting data (time-series aware)...\\n\",\n",
    "      \"‚úì Train: 3414, Test: 854\\n\",\n",
    "      \"\\n\",\n",
    "      \"6. Setting up hyperparameter grids...\\n\",\n",
    "      \"‚úì 6 models configured for tuning\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"TRAINING MODELS WITH HYPERPARAMETER TUNING\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"‚è±Ô∏è  This may take 5-15 minutes depending on your hardware...\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"TRAINING FOR 24h AHEAD PREDICTION\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"Ridge...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 100.0, 'solver': 'saga'}\\n\",\n",
    "      \"   Best CV R¬≤: 0.085\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.013\\n\",\n",
    "      \"      Train R¬≤:    0.168\\n\",\n",
    "      \"      CV R¬≤:       0.085\\n\",\n",
    "      \"      RMSE:        55.83\\n\",\n",
    "      \"      MAE:         28.41\\n\",\n",
    "      \"      MAPE:        24.99%\\n\",\n",
    "      \"      Acc ¬±20:      51.5%\\n\",\n",
    "      \"      Acc ¬±10:      23.4%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/ridge_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Lasso...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 0.1}\\n\",\n",
    "      \"   Best CV R¬≤: 0.076\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.009\\n\",\n",
    "      \"      Train R¬≤:    0.169\\n\",\n",
    "      \"      CV R¬≤:       0.076\\n\",\n",
    "      \"      RMSE:        55.94\\n\",\n",
    "      \"      MAE:         28.71\\n\",\n",
    "      \"      MAPE:        25.45%\\n\",\n",
    "      \"      Acc ¬±20:      52.3%\\n\",\n",
    "      \"      Acc ¬±10:      22.8%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/lasso_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Random Forest...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 100}\\n\",\n",
    "      \"   Best CV R¬≤: 0.042\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:    -0.009\\n\",\n",
    "      \"      Train R¬≤:    0.280\\n\",\n",
    "      \"      CV R¬≤:       0.042\\n\",\n",
    "      \"      RMSE:        56.44\\n\",\n",
    "      \"      MAE:         29.41\\n\",\n",
    "      \"      MAPE:        26.11%\\n\",\n",
    "      \"      Acc ¬±20:      49.2%\\n\",\n",
    "      \"      Acc ¬±10:      27.3%\\n\",\n",
    "      \"      ‚ö†Ô∏è  OVERFITTING (gap: 0.289)\\n\",\n",
    "      \"      ‚úì  Saved: models/random_forest_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Gradient Boosting...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: 0.040\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.035\\n\",\n",
    "      \"      Train R¬≤:    0.128\\n\",\n",
    "      \"      CV R¬≤:       0.040\\n\",\n",
    "      \"      RMSE:        55.20\\n\",\n",
    "      \"      MAE:         27.74\\n\",\n",
    "      \"      MAPE:        24.99%\\n\",\n",
    "      \"      Acc ¬±20:      50.5%\\n\",\n",
    "      \"      Acc ¬±10:      22.7%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/gradient_boosting_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"XGBoost...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: 0.066\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.036\\n\",\n",
    "      \"      Train R¬≤:    0.182\\n\",\n",
    "      \"      CV R¬≤:       0.066\\n\",\n",
    "      \"      RMSE:        55.16\\n\",\n",
    "      \"      MAE:         28.07\\n\",\n",
    "      \"      MAPE:        25.14%\\n\",\n",
    "      \"      Acc ¬±20:      49.9%\\n\",\n",
    "      \"      Acc ¬±10:      24.8%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/xgboost_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"LightGBM...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 7, 'min_child_samples': 30, 'n_estimators': 200, 'num_leaves': 31}\\n\",\n",
    "      \"   Best CV R¬≤: 0.052\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:    -0.013\\n\",\n",
    "      \"      Train R¬≤:    0.384\\n\",\n",
    "      \"      CV R¬≤:       0.052\\n\",\n",
    "      \"      RMSE:        56.54\\n\",\n",
    "      \"      MAE:         29.62\\n\",\n",
    "      \"      MAPE:        26.16%\\n\",\n",
    "      \"      Acc ¬±20:      49.1%\\n\",\n",
    "      \"      Acc ¬±10:      25.3%\\n\",\n",
    "      \"      ‚ö†Ô∏è  OVERFITTING (gap: 0.396)\\n\",\n",
    "      \"      ‚úì  Saved: models/lightgbm_24h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"TRAINING FOR 48h AHEAD PREDICTION\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"Ridge...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 100.0, 'solver': 'saga'}\\n\",\n",
    "      \"   Best CV R¬≤: -0.009\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.022\\n\",\n",
    "      \"      Train R¬≤:    0.115\\n\",\n",
    "      \"      CV R¬≤:      -0.009\\n\",\n",
    "      \"      RMSE:        58.69\\n\",\n",
    "      \"      MAE:         27.92\\n\",\n",
    "      \"      MAPE:        24.16%\\n\",\n",
    "      \"      Acc ¬±20:      51.3%\\n\",\n",
    "      \"      Acc ¬±10:      27.5%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/ridge_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Lasso...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 1.0}\\n\",\n",
    "      \"   Best CV R¬≤: 0.008\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.032\\n\",\n",
    "      \"      Train R¬≤:    0.104\\n\",\n",
    "      \"      CV R¬≤:       0.008\\n\",\n",
    "      \"      RMSE:        58.40\\n\",\n",
    "      \"      MAE:         27.86\\n\",\n",
    "      \"      MAPE:        24.30%\\n\",\n",
    "      \"      Acc ¬±20:      49.4%\\n\",\n",
    "      \"      Acc ¬±10:      27.4%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/lasso_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Random Forest...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 200}\\n\",\n",
    "      \"   Best CV R¬≤: -0.144\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.033\\n\",\n",
    "      \"      Train R¬≤:    0.286\\n\",\n",
    "      \"      CV R¬≤:      -0.144\\n\",\n",
    "      \"      RMSE:        58.37\\n\",\n",
    "      \"      MAE:         27.82\\n\",\n",
    "      \"      MAPE:        23.99%\\n\",\n",
    "      \"      Acc ¬±20:      50.9%\\n\",\n",
    "      \"      Acc ¬±10:      26.5%\\n\",\n",
    "      \"      ‚ö†Ô∏è  OVERFITTING (gap: 0.254)\\n\",\n",
    "      \"      ‚úì  Saved: models/random_forest_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Gradient Boosting...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: -0.014\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.018\\n\",\n",
    "      \"      Train R¬≤:    0.104\\n\",\n",
    "      \"      CV R¬≤:      -0.014\\n\",\n",
    "      \"      RMSE:        58.79\\n\",\n",
    "      \"      MAE:         28.06\\n\",\n",
    "      \"      MAPE:        24.40%\\n\",\n",
    "      \"      Acc ¬±20:      50.4%\\n\",\n",
    "      \"      Acc ¬±10:      27.9%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/gradient_boosting_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"XGBoost...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 50, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: -0.005\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.025\\n\",\n",
    "      \"      Train R¬≤:    0.178\\n\",\n",
    "      \"      CV R¬≤:      -0.005\\n\",\n",
    "      \"      RMSE:        58.60\\n\",\n",
    "      \"      MAE:         28.04\\n\",\n",
    "      \"      MAPE:        24.55%\\n\",\n",
    "      \"      Acc ¬±20:      50.9%\\n\",\n",
    "      \"      Acc ¬±10:      28.1%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/xgboost_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"LightGBM...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 3, 'min_child_samples': 10, 'n_estimators': 100, 'num_leaves': 15}\\n\",\n",
    "      \"   Best CV R¬≤: -0.025\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.025\\n\",\n",
    "      \"      Train R¬≤:    0.149\\n\",\n",
    "      \"      CV R¬≤:      -0.025\\n\",\n",
    "      \"      RMSE:        58.59\\n\",\n",
    "      \"      MAE:         27.60\\n\",\n",
    "      \"      MAPE:        23.48%\\n\",\n",
    "      \"      Acc ¬±20:      51.3%\\n\",\n",
    "      \"      Acc ¬±10:      28.6%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/lightgbm_48h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"TRAINING FOR 72h AHEAD PREDICTION\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"Ridge...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 100.0, 'solver': 'saga'}\\n\",\n",
    "      \"   Best CV R¬≤: -0.120\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.025\\n\",\n",
    "      \"      Train R¬≤:    0.098\\n\",\n",
    "      \"      CV R¬≤:      -0.120\\n\",\n",
    "      \"      RMSE:        58.61\\n\",\n",
    "      \"      MAE:         27.49\\n\",\n",
    "      \"      MAPE:        23.61%\\n\",\n",
    "      \"      Acc ¬±20:      54.1%\\n\",\n",
    "      \"      Acc ¬±10:      30.0%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/ridge_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Lasso...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'alpha': 1.0}\\n\",\n",
    "      \"   Best CV R¬≤: -0.046\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.033\\n\",\n",
    "      \"      Train R¬≤:    0.083\\n\",\n",
    "      \"      CV R¬≤:      -0.046\\n\",\n",
    "      \"      RMSE:        58.36\\n\",\n",
    "      \"      MAE:         27.41\\n\",\n",
    "      \"      MAPE:        23.87%\\n\",\n",
    "      \"      Acc ¬±20:      51.3%\\n\",\n",
    "      \"      Acc ¬±10:      27.5%\\n\",\n",
    "      \"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\n\",\n",
    "      \"      ‚úì  Saved: models/lasso_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Random Forest...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 200}\\n\",\n",
    "      \"   Best CV R¬≤: -0.291\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.015\\n\",\n",
    "      \"      Train R¬≤:    0.291\\n\",\n",
    "      \"      CV R¬≤:      -0.291\\n\",\n",
    "      \"      RMSE:        58.92\\n\",\n",
    "      \"      MAE:         27.45\\n\",\n",
    "      \"      MAPE:        23.53%\\n\",\n",
    "      \"      Acc ¬±20:      55.4%\\n\",\n",
    "      \"      Acc ¬±10:      28.5%\\n\",\n",
    "      \"      ‚ö†Ô∏è  OVERFITTING (gap: 0.277)\\n\",\n",
    "      \"      ‚úì  Saved: models/random_forest_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"Gradient Boosting...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: -0.258\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:    -0.002\\n\",\n",
    "      \"      Train R¬≤:    0.106\\n\",\n",
    "      \"      CV R¬≤:      -0.258\\n\",\n",
    "      \"      RMSE:        59.40\\n\",\n",
    "      \"      MAE:         28.39\\n\",\n",
    "      \"      MAPE:        24.68%\\n\",\n",
    "      \"      Acc ¬±20:      52.7%\\n\",\n",
    "      \"      Acc ¬±10:      27.8%\\n\",\n",
    "      \"      ‚ö†Ô∏è  NEGATIVE R¬≤ - Model performs worse than baseline!\\n\",\n",
    "      \"      ‚úì  Saved: models/gradient_boosting_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"XGBoost...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 50, 'subsample': 0.8}\\n\",\n",
    "      \"   Best CV R¬≤: -0.017\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:     0.012\\n\",\n",
    "      \"      Train R¬≤:    0.239\\n\",\n",
    "      \"      CV R¬≤:      -0.017\\n\",\n",
    "      \"      RMSE:        59.01\\n\",\n",
    "      \"      MAE:         28.17\\n\",\n",
    "      \"      MAPE:        24.68%\\n\",\n",
    "      \"      Acc ¬±20:      52.1%\\n\",\n",
    "      \"      Acc ¬±10:      28.0%\\n\",\n",
    "      \"      ‚ö†Ô∏è  OVERFITTING (gap: 0.227)\\n\",\n",
    "      \"      ‚úì  Saved: models/xgboost_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"LightGBM...\\n\",\n",
    "      \"   Tuning hyperparameters...\\n\",\n",
    "      \"   Best params: {'learning_rate': 0.01, 'max_depth': 3, 'min_child_samples': 10, 'n_estimators': 50, 'num_leaves': 15}\\n\",\n",
    "      \"   Best CV R¬≤: -0.038\\n\",\n",
    "      \"\\n\",\n",
    "      \"   üìä Results:\\n\",\n",
    "      \"      Test R¬≤:    -0.009\\n\",\n",
    "      \"      Train R¬≤:    0.100\\n\",\n",
    "      \"      CV R¬≤:      -0.038\\n\",\n",
    "      \"      RMSE:        59.63\\n\",\n",
    "      \"      MAE:         28.37\\n\",\n",
    "      \"      MAPE:        24.40%\\n\",\n",
    "      \"      Acc ¬±20:      53.9%\\n\",\n",
    "      \"      Acc ¬±10:      28.7%\\n\",\n",
    "      \"      ‚ö†Ô∏è  NEGATIVE R¬≤ - Model performs worse than baseline!\\n\",\n",
    "      \"      ‚úì  Saved: models/lightgbm_72h.pkl\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"FEATURE IMPORTANCE ANALYSIS\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"Best 24h model: XGBoost (R¬≤ = 0.036)\\n\",\n",
    "      \"\\n\",\n",
    "      \"Top 15 Most Important Features:\\n\",\n",
    "      \"   aqi                           : 0.1379\\n\",\n",
    "      \"   pm2_5                         : 0.1253\\n\",\n",
    "      \"   aqi_pm25                      : 0.0728\\n\",\n",
    "      \"   aqi_lag_3h                    : 0.0537\\n\",\n",
    "      \"   aqi_min_6h                    : 0.0520\\n\",\n",
    "      \"   aqi_ma_3h                     : 0.0468\\n\",\n",
    "      \"   aqi_std_24h                   : 0.0446\\n\",\n",
    "      \"   pm25_lag_1h                   : 0.0367\\n\",\n",
    "      \"   aqi_lag_1h                    : 0.0362\\n\",\n",
    "      \"   aqi_min_3h                    : 0.0343\\n\",\n",
    "      \"   aqi_max_12h                   : 0.0310\\n\",\n",
    "      \"   pres                          : 0.0305\\n\",\n",
    "      \"   aqi_ma_12h                    : 0.0296\\n\",\n",
    "      \"   aqi_min_24h                   : 0.0296\\n\",\n",
    "      \"   day_of_year                   : 0.0290\\n\",\n",
    "      \"\\n\",\n",
    "      \"‚úì Feature importance saved to: models/feature_importance.csv\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"RESULTS SUMMARY\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"24h Ahead:\\n\",\n",
    "      \"----------------------------------------------------------------------\\n\",\n",
    "      \"Ridge             : R¬≤=  0.013  RMSE= 55.83  MAE= 28.41  Acc¬±20= 51.5%\\n\",\n",
    "      \"Lasso             : R¬≤=  0.009  RMSE= 55.94  MAE= 28.71  Acc¬±20= 52.3%\\n\",\n",
    "      \"Random Forest     : R¬≤= -0.009  RMSE= 56.44  MAE= 29.41  Acc¬±20= 49.2%\\n\",\n",
    "      \"Gradient Boosting : R¬≤=  0.035  RMSE= 55.20  MAE= 27.74  Acc¬±20= 50.5%\\n\",\n",
    "      \"XGBoost           : R¬≤=  0.036  RMSE= 55.16  MAE= 28.07  Acc¬±20= 49.9% ‚òÖ BEST\\n\",\n",
    "      \"LightGBM          : R¬≤= -0.013  RMSE= 56.54  MAE= 29.62  Acc¬±20= 49.1%\\n\",\n",
    "      \"\\n\",\n",
    "      \"48h Ahead:\\n\",\n",
    "      \"----------------------------------------------------------------------\\n\",\n",
    "      \"Ridge             : R¬≤=  0.022  RMSE= 58.69  MAE= 27.92  Acc¬±20= 51.3%\\n\",\n",
    "      \"Lasso             : R¬≤=  0.032  RMSE= 58.40  MAE= 27.86  Acc¬±20= 49.4%\\n\",\n",
    "      \"Random Forest     : R¬≤=  0.033  RMSE= 58.37  MAE= 27.82  Acc¬±20= 50.9% ‚òÖ BEST\\n\",\n",
    "      \"Gradient Boosting : R¬≤=  0.018  RMSE= 58.79  MAE= 28.06  Acc¬±20= 50.4%\\n\",\n",
    "      \"XGBoost           : R¬≤=  0.025  RMSE= 58.60  MAE= 28.04  Acc¬±20= 50.9%\\n\",\n",
    "      \"LightGBM          : R¬≤=  0.025  RMSE= 58.59  MAE= 27.60  Acc¬±20= 51.3%\\n\",\n",
    "      \"\\n\",\n",
    "      \"72h Ahead:\\n\",\n",
    "      \"----------------------------------------------------------------------\\n\",\n",
    "      \"Ridge             : R¬≤=  0.025  RMSE= 58.61  MAE= 27.49  Acc¬±20= 54.1%\\n\",\n",
    "      \"Lasso             : R¬≤=  0.033  RMSE= 58.36  MAE= 27.41  Acc¬±20= 51.3% ‚òÖ BEST\\n\",\n",
    "      \"Random Forest     : R¬≤=  0.015  RMSE= 58.92  MAE= 27.45  Acc¬±20= 55.4%\\n\",\n",
    "      \"Gradient Boosting : R¬≤= -0.002  RMSE= 59.40  MAE= 28.39  Acc¬±20= 52.7%\\n\",\n",
    "      \"XGBoost           : R¬≤=  0.012  RMSE= 59.01  MAE= 28.17  Acc¬±20= 52.1%\\n\",\n",
    "      \"LightGBM          : R¬≤= -0.009  RMSE= 59.63  MAE= 28.37  Acc¬±20= 53.9%\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"‚úÖ TRAINING COMPLETE!\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"üìä Source: MONGODB\\n\",\n",
    "      \"üìà Models: 6 ML models √ó 3 horizons = 18 total\\n\",\n",
    "      \"üéØ All models hyperparameter-tuned with GridSearchCV\\n\",\n",
    "      \"\\n\",\n",
    "      \"üìÅ Saved:\\n\",\n",
    "      \"  ‚úì models/*.pkl (tuned models)\\n\",\n",
    "      \"  ‚úì models/scaler_ml.pkl\\n\",\n",
    "      \"  ‚úì models/ml_tuned_results.json\\n\",\n",
    "      \"  ‚úì models/feature_importance.csv\\n\",\n",
    "      \"\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"DIAGNOSTIC INFORMATION\\n\",\n",
    "      \"======================================================================\\n\",\n",
    "      \"\\n\",\n",
    "      \"Data Quality:\\n\",\n",
    "      \"   Total samples: 4268\\n\",\n",
    "      \"   Training samples: 3414\\n\",\n",
    "      \"   Test samples: 854\\n\",\n",
    "      \"   Features used: 30\\n\",\n",
    "      \"\\n\",\n",
    "      \"Target Statistics (24h ahead):\\n\",\n",
    "      \"   Mean: 101.86\\n\",\n",
    "      \"   Std: 55.66\\n\",\n",
    "      \"   Min: 29.00\\n\",\n",
    "      \"   Max: 500.00\\n\",\n",
    "      \"\\n\",\n",
    "      \"Best Model Performance:\\n\",\n",
    "      \"   XGBoost (24h): R¬≤ = 0.036\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, RobustScaler\\n\",\n",
    "    \"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\\n\",\n",
    "    \"from sklearn.linear_model import Ridge, Lasso, ElasticNet\\n\",\n",
    "    \"from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\\n\",\n",
    "    \"import xgboost as xgb\\n\",\n",
    "    \"import lightgbm as lgb\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"ADVANCED AQI PREDICTION - WITH HYPERPARAMETER TUNING\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"os.makedirs('models', exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 1. Load Data\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_from_mongodb(uri, max_attempts=2):\\n\",\n",
    "    \"    \\\"\\\"\\\"Try MongoDB\\\"\\\"\\\"\\n\",\n",
    "    \"    from pymongo import MongoClient\\n\",\n",
    "    \"    from pymongo.server_api import ServerApi\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for attempt in range(max_attempts):\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            print(f\\\"\\\\nAttempt {attempt + 1}/{max_attempts}: Connecting to MongoDB...\\\")\\n\",\n",
    "    \"            client = MongoClient(uri, server_api=ServerApi('1'),\\n\",\n",
    "    \"                               serverSelectionTimeoutMS=5000, connectTimeoutMS=5000)\\n\",\n",
    "    \"            client.admin.command('ping')\\n\",\n",
    "    \"            print(\\\"‚úì Connected!\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            db = client['aqi_feature_store']\\n\",\n",
    "    \"            collection = db['aqi_features']\\n\",\n",
    "    \"            data = pd.DataFrame(list(collection.find({}, {\\\"_id\\\": 0})))\\n\",\n",
    "    \"            client.close()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"‚úì Loaded {len(data)} records from MongoDB\\\")\\n\",\n",
    "    \"            return data, 'mongodb'\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"‚úó Failed: {str(e)[:80]}\\\")\\n\",\n",
    "    \"    return None, None\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_from_csv(csv_path):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load from CSV\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        print(f\\\"\\\\nLoading from CSV: {csv_path}\\\")\\n\",\n",
    "    \"        data = pd.read_csv(csv_path)\\n\",\n",
    "    \"        print(f\\\"‚úì Loaded {len(data)} records\\\")\\n\",\n",
    "    \"        return data, 'csv'\\n\",\n",
    "    \"    except:\\n\",\n",
    "    \"        return None, None\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n1. Loading data...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"MONGO_URI = \\\"mongodb+srv://nawababbas08_db_user:2Ja4OGlDdKfG6EvZ@cluster0.jnxn95g.mongodb.net/?retryWrites=true&w=majority&tlsAllowInvalidCertificates=true\\\"\\n\",\n",
    "    \"CSV_PATH = \\\"data/cleaned_aqi_data_v2.csv\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"data, source = load_from_mongodb(MONGO_URI, 2)\\n\",\n",
    "    \"if data is None:\\n\",\n",
    "    \"    print(\\\"\\\\n‚ö†Ô∏è MongoDB failed, using CSV...\\\")\\n\",\n",
    "    \"    data, source = load_from_csv(CSV_PATH)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if data is None:\\n\",\n",
    "    \"    print(\\\"\\\\n‚úó ERROR: No data source available\\\")\\n\",\n",
    "    \"    exit(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úì Source: {source.upper()}\\\")\\n\",\n",
    "    \"print(f\\\"‚úì Records: {len(data)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 2. Enhanced Feature Engineering\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n2. Engineering features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'time' in data.columns:\\n\",\n",
    "    \"    data['time'] = pd.to_datetime(data['time'])\\n\",\n",
    "    \"    data = data.sort_values('time').reset_index(drop=True)\\n\",\n",
    "    \"elif 'timestamp' in data.columns:\\n\",\n",
    "    \"    data['timestamp'] = pd.to_datetime(data['timestamp'])\\n\",\n",
    "    \"    data = data.sort_values('timestamp').reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# More comprehensive lag features\\n\",\n",
    "    \"print(\\\"   Creating lag features...\\\")\\n\",\n",
    "    \"for lag in [1, 2, 3, 6, 12, 24, 48]:\\n\",\n",
    "    \"    if 'aqi' in data.columns:\\n\",\n",
    "    \"        data[f'aqi_lag_{lag}h'] = data['aqi'].shift(lag)\\n\",\n",
    "    \"    if 'pm2_5' in data.columns:\\n\",\n",
    "    \"        data[f'pm25_lag_{lag}h'] = data['pm2_5'].shift(lag)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Rolling statistics (mean, std, min, max)\\n\",\n",
    "    \"print(\\\"   Creating rolling features...\\\")\\n\",\n",
    "    \"for window in [3, 6, 12, 24]:\\n\",\n",
    "    \"    if 'aqi' in data.columns:\\n\",\n",
    "    \"        data[f'aqi_ma_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).mean()\\n\",\n",
    "    \"        data[f'aqi_std_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).std()\\n\",\n",
    "    \"        data[f'aqi_min_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).min()\\n\",\n",
    "    \"        data[f'aqi_max_{window}h'] = data['aqi'].rolling(window=window, min_periods=1).max()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Difference features (trend detection)\\n\",\n",
    "    \"print(\\\"   Creating difference features...\\\")\\n\",\n",
    "    \"if 'aqi' in data.columns:\\n\",\n",
    "    \"    data['aqi_diff_1h'] = data['aqi'].diff(1)\\n\",\n",
    "    \"    data['aqi_diff_3h'] = data['aqi'].diff(3)\\n\",\n",
    "    \"    data['aqi_diff_24h'] = data['aqi'].diff(24)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Cyclical features (better encoding)\\n\",\n",
    "    \"if 'hour' in data.columns:\\n\",\n",
    "    \"    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\\n\",\n",
    "    \"    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'day_of_week' in data.columns:\\n\",\n",
    "    \"    data['dow_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\\n\",\n",
    "    \"    data['dow_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'month' in data.columns:\\n\",\n",
    "    \"    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\\n\",\n",
    "    \"    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Targets\\n\",\n",
    "    \"data['aqi_24h'] = data['aqi'].shift(-24)\\n\",\n",
    "    \"data['aqi_48h'] = data['aqi'].shift(-48)\\n\",\n",
    "    \"data['aqi_72h'] = data['aqi'].shift(-72)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove rows with all NaN\\n\",\n",
    "    \"data = data.dropna(axis=1, how='all')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì After engineering: {data.shape[0]} records, {data.shape[1]} columns\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 3. Prepare Data with Better Filtering\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n3. Preparing features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Exclude target columns and categorical/string columns\\n\",\n",
    "    \"exclude_cols = ['time', 'timestamp', 'aqi_24h', 'aqi_48h', 'aqi_72h',\\n\",\n",
    "    \"                'dominant_pollutant', 'aqi_category', 'aqi_color', 'time_of_day',\\n\",\n",
    "    \"                'season', 'weather_condition', 'day_of_week', 'day_of_month',\\n\",\n",
    "    \"                'is_weekend']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get only numeric columns for features\\n\",\n",
    "    \"feature_cols = [col for col in data.columns if col not in exclude_cols]\\n\",\n",
    "    \"numeric_cols = data[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\\n\",\n",
    "    \"feature_cols = numeric_cols\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì Initial features: {len(feature_cols)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove features with too many missing values\\n\",\n",
    "    \"missing_threshold = 0.3\\n\",\n",
    "    \"for col in feature_cols[:]:\\n\",\n",
    "    \"    missing_pct = data[col].isnull().sum() / len(data)\\n\",\n",
    "    \"    if missing_pct > missing_threshold:\\n\",\n",
    "    \"        feature_cols.remove(col)\\n\",\n",
    "    \"        print(f\\\"   Removed {col} (missing: {missing_pct*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì After removing high-missing features: {len(feature_cols)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Handle remaining missing values\\n\",\n",
    "    \"data[feature_cols] = data[feature_cols].fillna(data[feature_cols].mean())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove rows where target is missing\\n\",\n",
    "    \"data = data.dropna(subset=['aqi_24h', 'aqi_48h', 'aqi_72h'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì Final dataset: {len(data)} records\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"X = data[feature_cols]\\n\",\n",
    "    \"y_24h = data['aqi_24h']\\n\",\n",
    "    \"y_48h = data['aqi_48h']\\n\",\n",
    "    \"y_72h = data['aqi_72h']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove any remaining NaN\\n\",\n",
    "    \"X = X.fillna(X.mean())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì Features: {len(feature_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"‚úì Samples: {len(X)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 4. Feature Selection\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n4. Feature selection...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"def select_best_features(X, y, k=30):\\n\",\n",
    "    \"    \\\"\\\"\\\"Select top K most important features\\\"\\\"\\\"\\n\",\n",
    "    \"    if len(X.columns) <= k:\\n\",\n",
    "    \"        return X.columns.tolist()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Use mutual information for feature selection\\n\",\n",
    "    \"    selector = SelectKBest(score_func=mutual_info_regression, k=k)\\n\",\n",
    "    \"    selector.fit(X, y)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get selected feature names\\n\",\n",
    "    \"    selected_features = X.columns[selector.get_support()].tolist()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get scores\\n\",\n",
    "    \"    scores = selector.scores_\\n\",\n",
    "    \"    feature_scores = list(zip(X.columns, scores))\\n\",\n",
    "    \"    feature_scores.sort(key=lambda x: x[1], reverse=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   Top 10 features:\\\")\\n\",\n",
    "    \"    for feat, score in feature_scores[:10]:\\n\",\n",
    "    \"        print(f\\\"      {feat:30s}: {score:.3f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return selected_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select features for 24h prediction\\n\",\n",
    "    \"selected_features = select_best_features(X, y_24h, k=min(30, len(X.columns)))\\n\",\n",
    "    \"X = X[selected_features]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úì Selected {len(selected_features)} best features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 5. Time Series Split (Better for Time Series!)\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n5. Splitting data (time-series aware)...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use 80-20 split but maintain time order\\n\",\n",
    "    \"split_idx = int(len(X) * 0.8)\\n\",\n",
    "    \"X_train, X_test = X[:split_idx], X[split_idx:]\\n\",\n",
    "    \"y_24h_train, y_24h_test = y_24h[:split_idx], y_24h[split_idx:]\\n\",\n",
    "    \"y_48h_train, y_48h_test = y_48h[:split_idx], y_48h[split_idx:]\\n\",\n",
    "    \"y_72h_train, y_72h_test = y_72h[:split_idx], y_72h[split_idx:]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use RobustScaler (better for outliers)\\n\",\n",
    "    \"scaler = RobustScaler()\\n\",\n",
    "    \"X_train_scaled = scaler.fit_transform(X_train)\\n\",\n",
    "    \"X_test_scaled = scaler.transform(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('models/scaler_ml.pkl', 'wb') as f:\\n\",\n",
    "    \"    pickle.dump(scaler, f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì Train: {len(X_train)}, Test: {len(X_test)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 6. Define Model Hyperparameter Grids\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n6. Setting up hyperparameter grids...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Simplified grids for faster training\\n\",\n",
    "    \"param_grids = {\\n\",\n",
    "    \"    'Ridge': {\\n\",\n",
    "    \"        'model': Ridge(),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'alpha': [0.1, 1.0, 10.0, 100.0],\\n\",\n",
    "    \"            'solver': ['auto', 'svd', 'saga']\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'Lasso': {\\n\",\n",
    "    \"        'model': Lasso(max_iter=5000),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'alpha': [0.01, 0.1, 1.0, 10.0]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'Random Forest': {\\n\",\n",
    "    \"        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'n_estimators': [50, 100, 200],\\n\",\n",
    "    \"            'max_depth': [5, 10, 15, None],\\n\",\n",
    "    \"            'min_samples_split': [5, 10, 20],\\n\",\n",
    "    \"            'min_samples_leaf': [2, 4, 8]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'Gradient Boosting': {\\n\",\n",
    "    \"        'model': GradientBoostingRegressor(random_state=42),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'n_estimators': [50, 100, 200],\\n\",\n",
    "    \"            'learning_rate': [0.01, 0.05, 0.1],\\n\",\n",
    "    \"            'max_depth': [3, 5, 7],\\n\",\n",
    "    \"            'subsample': [0.8, 1.0]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'XGBoost': {\\n\",\n",
    "    \"        'model': xgb.XGBRegressor(random_state=42, n_jobs=-1),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'n_estimators': [50, 100, 200],\\n\",\n",
    "    \"            'learning_rate': [0.01, 0.05, 0.1],\\n\",\n",
    "    \"            'max_depth': [3, 5, 7],\\n\",\n",
    "    \"            'min_child_weight': [1, 3, 5],\\n\",\n",
    "    \"            'subsample': [0.8, 1.0],\\n\",\n",
    "    \"            'colsample_bytree': [0.8, 1.0]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'LightGBM': {\\n\",\n",
    "    \"        'model': lgb.LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1),\\n\",\n",
    "    \"        'params': {\\n\",\n",
    "    \"            'n_estimators': [50, 100, 200],\\n\",\n",
    "    \"            'learning_rate': [0.01, 0.05, 0.1],\\n\",\n",
    "    \"            'max_depth': [3, 5, 7],\\n\",\n",
    "    \"            'num_leaves': [15, 31, 63],\\n\",\n",
    "    \"            'min_child_samples': [10, 20, 30]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úì {len(param_grids)} models configured for tuning\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 7. Evaluation Function\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"def evaluate(y_true, y_pred):\\n\",\n",
    "    \"    \\\"\\\"\\\"Comprehensive evaluation metrics\\\"\\\"\\\"\\n\",\n",
    "    \"    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\\n\",\n",
    "    \"    mae = mean_absolute_error(y_true, y_pred)\\n\",\n",
    "    \"    r2 = r2_score(y_true, y_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Accuracy within thresholds\\n\",\n",
    "    \"    acc_20 = np.sum(np.abs(y_true - y_pred) <= 20) / len(y_true) * 100\\n\",\n",
    "    \"    acc_10 = np.sum(np.abs(y_true - y_pred) <= 10) / len(y_true) * 100\\n\",\n",
    "    \"    acc_5 = np.sum(np.abs(y_true - y_pred) <= 5) / len(y_true) * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # MAPE (Mean Absolute Percentage Error)\\n\",\n",
    "    \"    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'RMSE': rmse,\\n\",\n",
    "    \"        'MAE': mae,\\n\",\n",
    "    \"        'R2': r2,\\n\",\n",
    "    \"        'MAPE': mape,\\n\",\n",
    "    \"        'Acc20': acc_20,\\n\",\n",
    "    \"        'Acc10': acc_10,\\n\",\n",
    "    \"        'Acc5': acc_5\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 8. Training with GridSearch\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"TRAINING MODELS WITH HYPERPARAMETER TUNING\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"\\\\n‚è±Ô∏è  This may take 5-15 minutes depending on your hardware...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Time series cross-validation\\n\",\n",
    "    \"tscv = TimeSeriesSplit(n_splits=3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for horizon, y_train, y_test in [\\n\",\n",
    "    \"    ('24h', y_24h_train, y_24h_test),\\n\",\n",
    "    \"    ('48h', y_48h_train, y_48h_test),\\n\",\n",
    "    \"    ('72h', y_72h_train, y_72h_test)\\n\",\n",
    "    \"]:\\n\",\n",
    "    \"    print(f\\\"\\\\n{'='*70}\\\")\\n\",\n",
    "    \"    print(f\\\"TRAINING FOR {horizon} AHEAD PREDICTION\\\")\\n\",\n",
    "    \"    print('='*70)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results[horizon] = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for name, config in param_grids.items():\\n\",\n",
    "    \"        print(f\\\"\\\\n{name}...\\\")\\n\",\n",
    "    \"        print(f\\\"   Tuning hyperparameters...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GridSearch with time series CV\\n\",\n",
    "    \"        grid_search = GridSearchCV(\\n\",\n",
    "    \"            estimator=config['model'],\\n\",\n",
    "    \"            param_grid=config['params'],\\n\",\n",
    "    \"            cv=tscv,\\n\",\n",
    "    \"            scoring='r2',\\n\",\n",
    "    \"            n_jobs=-1,\\n\",\n",
    "    \"            verbose=0\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fit grid search\\n\",\n",
    "    \"        grid_search.fit(X_train_scaled, y_train)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Best model\\n\",\n",
    "    \"        best_model = grid_search.best_estimator_\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"   Best params: {grid_search.best_params_}\\\")\\n\",\n",
    "    \"        print(f\\\"   Best CV R¬≤: {grid_search.best_score_:.3f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Predictions\\n\",\n",
    "    \"        y_pred_train = best_model.predict(X_train_scaled)\\n\",\n",
    "    \"        y_pred_test = best_model.predict(X_test_scaled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Evaluate\\n\",\n",
    "    \"        train_metrics = evaluate(y_train, y_pred_train)\\n\",\n",
    "    \"        test_metrics = evaluate(y_test, y_pred_test)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Store results\\n\",\n",
    "    \"        results[horizon][name] = {\\n\",\n",
    "    \"            'test_R2': test_metrics['R2'],\\n\",\n",
    "    \"            'test_RMSE': test_metrics['RMSE'],\\n\",\n",
    "    \"            'test_MAE': test_metrics['MAE'],\\n\",\n",
    "    \"            'test_MAPE': test_metrics['MAPE'],\\n\",\n",
    "    \"            'test_Acc20': test_metrics['Acc20'],\\n\",\n",
    "    \"            'test_Acc10': test_metrics['Acc10'],\\n\",\n",
    "    \"            'test_Acc5': test_metrics['Acc5'],\\n\",\n",
    "    \"            'train_R2': train_metrics['R2'],\\n\",\n",
    "    \"            'cv_R2': grid_search.best_score_,\\n\",\n",
    "    \"            'best_params': grid_search.best_params_\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display metrics\\n\",\n",
    "    \"        print(f\\\"\\\\n   üìä Results:\\\")\\n\",\n",
    "    \"        print(f\\\"      Test R¬≤:    {test_metrics['R2']:6.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"      Train R¬≤:   {train_metrics['R2']:6.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"      CV R¬≤:      {grid_search.best_score_:6.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"      RMSE:       {test_metrics['RMSE']:6.2f}\\\")\\n\",\n",
    "    \"        print(f\\\"      MAE:        {test_metrics['MAE']:6.2f}\\\")\\n\",\n",
    "    \"        print(f\\\"      MAPE:       {test_metrics['MAPE']:6.2f}%\\\")\\n\",\n",
    "    \"        print(f\\\"      Acc ¬±20:    {test_metrics['Acc20']:6.1f}%\\\")\\n\",\n",
    "    \"        print(f\\\"      Acc ¬±10:    {test_metrics['Acc10']:6.1f}%\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Check for overfitting\\n\",\n",
    "    \"        overfit_gap = train_metrics['R2'] - test_metrics['R2']\\n\",\n",
    "    \"        if overfit_gap > 0.2:\\n\",\n",
    "    \"            print(f\\\"      ‚ö†Ô∏è  OVERFITTING (gap: {overfit_gap:.3f})\\\")\\n\",\n",
    "    \"        elif test_metrics['R2'] < 0:\\n\",\n",
    "    \"            print(f\\\"      ‚ö†Ô∏è  NEGATIVE R¬≤ - Model performs worse than baseline!\\\")\\n\",\n",
    "    \"        elif test_metrics['R2'] < 0.1:\\n\",\n",
    "    \"            print(f\\\"      ‚ö†Ô∏è  VERY LOW R¬≤ - Check data quality\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"      ‚úì  Good performance!\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save model\\n\",\n",
    "    \"        model_path = f'models/{name.lower().replace(\\\" \\\", \\\"_\\\")}_{horizon}.pkl'\\n\",\n",
    "    \"        with open(model_path, 'wb') as f:\\n\",\n",
    "    \"            pickle.dump(best_model, f)\\n\",\n",
    "    \"        print(f\\\"      ‚úì  Saved: {model_path}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 9. Feature Importance Analysis\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"FEATURE IMPORTANCE ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze feature importance for best model\\n\",\n",
    "    \"best_24h = max(results['24h'].items(), key=lambda x: x[1]['test_R2'])\\n\",\n",
    "    \"print(f\\\"\\\\nBest 24h model: {best_24h[0]} (R¬≤ = {best_24h[1]['test_R2']:.3f})\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save feature importance if available\\n\",\n",
    "    \"feature_importance_path = 'models/feature_importance.csv'\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Load the best model\\n\",\n",
    "    \"    model_name = best_24h[0].lower().replace(\\\" \\\", \\\"_\\\")\\n\",\n",
    "    \"    with open(f'models/{model_name}_24h.pkl', 'rb') as f:\\n\",\n",
    "    \"        best_model = pickle.load(f)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get feature importance\\n\",\n",
    "    \"    if hasattr(best_model, 'feature_importances_'):\\n\",\n",
    "    \"        importance_df = pd.DataFrame({\\n\",\n",
    "    \"            'feature': selected_features,\\n\",\n",
    "    \"            'importance': best_model.feature_importances_\\n\",\n",
    "    \"        }).sort_values('importance', ascending=False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        importance_df.to_csv(feature_importance_path, index=False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nTop 15 Most Important Features:\\\")\\n\",\n",
    "    \"        for idx, row in importance_df.head(15).iterrows():\\n\",\n",
    "    \"            print(f\\\"   {row['feature']:30s}: {row['importance']:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\n‚úì Feature importance saved to: {feature_importance_path}\\\")\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"\\\\n‚ö†Ô∏è Could not extract feature importance: {str(e)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 10. Summary\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"RESULTS SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for horizon in ['24h', '48h', '72h']:\\n\",\n",
    "    \"    print(f\\\"\\\\n{horizon} Ahead:\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 70)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    best = max(results[horizon].items(), key=lambda x: x[1]['test_R2'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for name in results[horizon]:\\n\",\n",
    "    \"        m = results[horizon][name]\\n\",\n",
    "    \"        marker = \\\" ‚òÖ BEST\\\" if name == best[0] else \\\"\\\"\\n\",\n",
    "    \"        print(f\\\"{name:18s}: R¬≤={m['test_R2']:7.3f}  RMSE={m['test_RMSE']:6.2f}  \\\"\\n\",\n",
    "    \"              f\\\"MAE={m['test_MAE']:6.2f}  Acc¬±20={m['test_Acc20']:5.1f}%{marker}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save results\\n\",\n",
    "    \"with open('models/ml_tuned_results.json', 'w') as f:\\n\",\n",
    "    \"    # Convert to serializable format\\n\",\n",
    "    \"    results_serializable = {}\\n\",\n",
    "    \"    for horizon in results:\\n\",\n",
    "    \"        results_serializable[horizon] = {}\\n\",\n",
    "    \"        for model_name in results[horizon]:\\n\",\n",
    "    \"            results_serializable[horizon][model_name] = {\\n\",\n",
    "    \"                k: v for k, v in results[horizon][model_name].items() \\n\",\n",
    "    \"                if k != 'best_params'\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            results_serializable[horizon][model_name]['best_params_str'] = str(\\n\",\n",
    "    \"                results[horizon][model_name]['best_params']\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    json.dump(results_serializable, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"‚úÖ TRAINING COMPLETE!\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Source: {source.upper()}\\\")\\n\",\n",
    "    \"print(f\\\"üìà Models: {len(param_grids)} ML models √ó 3 horizons = {len(param_grids)*3} total\\\")\\n\",\n",
    "    \"print(f\\\"üéØ All models hyperparameter-tuned with GridSearchCV\\\")\\n\",\n",
    "    \"print(\\\"\\\\nüìÅ Saved:\\\")\\n\",\n",
    "    \"print(\\\"  ‚úì models/*.pkl (tuned models)\\\")\\n\",\n",
    "    \"print(\\\"  ‚úì models/scaler_ml.pkl\\\")\\n\",\n",
    "    \"print(\\\"  ‚úì models/ml_tuned_results.json\\\")\\n\",\n",
    "    \"print(\\\"  ‚úì models/feature_importance.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"# 11. Diagnostic Information\\n\",\n",
    "    \"# ============================================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"DIAGNOSTIC INFORMATION\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nData Quality:\\\")\\n\",\n",
    "    \"print(f\\\"   Total samples: {len(data)}\\\")\\n\",\n",
    "    \"print(f\\\"   Training samples: {len(X_train)}\\\")\\n\",\n",
    "    \"print(f\\\"   Test samples: {len(X_test)}\\\")\\n\",\n",
    "    \"print(f\\\"   Features used: {len(selected_features)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nTarget Statistics (24h ahead):\\\")\\n\",\n",
    "    \"print(f\\\"   Mean: {y_24h.mean():.2f}\\\")\\n\",\n",
    "    \"print(f\\\"   Std: {y_24h.std():.2f}\\\")\\n\",\n",
    "    \"print(f\\\"   Min: {y_24h.min():.2f}\\\")\\n\",\n",
    "    \"print(f\\\"   Max: {y_24h.max():.2f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nBest Model Performance:\\\")\\n\",\n",
    "    \"best_overall = max(\\n\",\n",
    "    \"    [(h, n, m['test_R2']) for h in results for n, m in results[h].items()],\\n\",\n",
    "    \"    key=lambda x: x[2]\\n\",\n",
    "    \")\\n\",\n",
    "    \"print(f\\\"   {best_overall[1]} ({best_overall[0]}): R¬≤ = {best_overall[2]:.3f}\\\")\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"dc732e3e-2aa4-4713-a012-1ca6fa0722b0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.14.2\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
