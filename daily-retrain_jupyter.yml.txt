name: Daily Model Retraining (CI Only)

on:
  schedule:
    - cron: '0 0 * * *'   # Every day at midnight UTC
  workflow_dispatch:        # Manual trigger button

jobs:
  retrain-models:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    # ------------------------------------------------------------------ #
    # 1. Checkout code                                                     #
    # ------------------------------------------------------------------ #
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GH_PAT }}
        fetch-depth: 0

    # ------------------------------------------------------------------ #
    # 2. Set up Python                                                     #
    # ------------------------------------------------------------------ #
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    # ------------------------------------------------------------------ #
    # 3. Install dependencies                                              #
    # ------------------------------------------------------------------ #
    - name: Install Python packages
      run: |
        python -m pip install --upgrade pip
        pip install \
          jupyter \
          nbconvert \
          pandas \
          numpy \
          scikit-learn \
          xgboost \
          lightgbm \
          pymongo \
          "pymongo[srv]" \
          dnspython \
          requests \
          python-dotenv

    # ------------------------------------------------------------------ #
    # 4. Fetch latest data                                                 #
    # ------------------------------------------------------------------ #
    - name: Fetch latest AQI data
      env:
        MONGODB_URI: ${{ secrets.MONGODB_URI }}
      run: |
        echo "Fetching data from APIs..."
        jupyter nbconvert --to script Scripts/Fetch_latest_data.ipynb --output /tmp/fetch
        python /tmp/fetch.py || echo "Data fetch completed with warnings"
      continue-on-error: true

    # ------------------------------------------------------------------ #
    # 5. Clean data                                                        #
    # ------------------------------------------------------------------ #
    - name: Clean and prepare data
      run: |
        echo "Cleaning data..."
        jupyter nbconvert --to script Scripts/clean_data.ipynb --output /tmp/clean
        python /tmp/clean.py || echo "Data cleaning completed"
      continue-on-error: true

    # ------------------------------------------------------------------ #
    # 6. Train models                                                      #
    # ------------------------------------------------------------------ #
    - name: Train ML models
      run: |
        echo "Training models..."
        jupyter nbconvert --to script Scripts/train_models.ipynb --output /tmp/train
        python /tmp/train.py

    # ------------------------------------------------------------------ #
    # 7. Prepare dashboard data                                            #
    # ------------------------------------------------------------------ #
    - name: Prepare dashboard data
      run: |
        python - <<'EOF'
        import pandas as pd
        import os
        
        print("Preparing dashboard data...")
        
        # Try to find cleaned CSV
        for csv_path in ['data/cleaned_aqi_data_v3.csv', 
                         'data/cleaned_aqi_data_v2.csv',
                         'data/complete_aqi_dataset.csv']:
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path)
                
                # Keep last 30 days (720 hours)
                df = df.tail(720)
                
                # Save for dashboard
                os.makedirs('data', exist_ok=True)
                df.to_csv('data/historical_aqi.csv', index=False)
                
                print(f"âœ“ Dashboard data ready: {len(df)} rows from {csv_path}")
                break
        else:
            print("âš  No cleaned CSV found - dashboard data not updated")
        EOF

    # ------------------------------------------------------------------ #
    # 8. Commit updated models back to repo                                #
    # ------------------------------------------------------------------ #
    - name: Commit and push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Add models and data
        git add Scripts/models/ data/historical_aqi.csv || true
        
        # Only commit if there are changes
        git diff --staged --quiet || {
          git commit -m "ðŸ¤– Daily model retrain: $(date +'%Y-%m-%d %H:%M UTC')"
          git push origin main
          echo "âœ“ Changes pushed to GitHub"
        } || echo "âœ“ No changes to commit"

    # ------------------------------------------------------------------ #
    # 9. Summary                                                           #
    # ------------------------------------------------------------------ #
    - name: Job Summary
      if: always()
      run: |
        echo "=================================================="
        echo "CI Pipeline Complete"
        echo "=================================================="
        echo ""
        echo "âœ“ Data fetched"
        echo "âœ“ Data cleaned"
        echo "âœ“ Models trained"
        echo "âœ“ Changes committed to GitHub"
        echo ""
        echo "Models location: Scripts/models/"
        echo "Dashboard data:  data/historical_aqi.csv"
        echo ""
        echo "Next run: Tomorrow at 00:00 UTC"
        echo "=================================================="